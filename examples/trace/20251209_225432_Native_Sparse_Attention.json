{
  "meta": {
    "source": "samples\\Native_Sparse_Attention.pdf",
    "timestamp": "20251209_225432"
  },
  "public_brief": {
    "title": "InterLines - Public Brief",
    "summary": "NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.\n\nNSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.\n\nThe paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.",
    "sections": [
      {
        "heading": "Key Insights",
        "body": "Generated from expert explanations.",
        "bullets": [
          "NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.",
          "NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.",
          "The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches."
        ]
      }
    ]
  },
  "public_brief_md_path": "artifacts\\reports\\latest.md",
  "traces": [
    {
      "note": "planner: phase 1 plan",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        }
      },
      "timestamp": "2025-12-10T04:51:36.965272Z",
      "revision": 4
    },
    {
      "note": "parser_agent (LLM): 20 segments -> parsed_chunks",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ]
      },
      "timestamp": "2025-12-10T04:52:28.367090Z",
      "revision": 5
    },
    {
      "note": "step 'translate' finished",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ],
        "explanations": [
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.', rationale='The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.', evidence=[EvidenceItem(text='NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.', rationale='Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.', evidence=[EvidenceItem(text='The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.', rationale='The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.', evidence=[EvidenceItem(text='Problem motivation and limitations of existing sparse attention', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA\u2019s overall framework and three-branch sparse attention design', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Detailed algorithmic components: compression, selection, sliding window, and gating', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Experimental setup and backbone architecture', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on general benchmarks', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on long-context benchmarks and retrieval', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Chain-of-thought reasoning results', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency measurements: training, prefill, and decoding', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Relation to prior sparse attention methods and design choices', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Training behavior and explored but discarded selection strategies', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Limitations and future directions', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)"
        ]
      },
      "timestamp": "2025-12-10T04:52:55.218543Z",
      "revision": 6
    },
    {
      "note": "step 'jargon' finished",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ],
        "explanations": [
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.', rationale='The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.', evidence=[EvidenceItem(text='NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.', rationale='Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.', evidence=[EvidenceItem(text='The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.', rationale='The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.', evidence=[EvidenceItem(text='Problem motivation and limitations of existing sparse attention', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA\u2019s overall framework and three-branch sparse attention design', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Detailed algorithmic components: compression, selection, sliding window, and gating', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Experimental setup and backbone architecture', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on general benchmarks', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on long-context benchmarks and retrieval', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Chain-of-thought reasoning results', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency measurements: training, prefill, and decoding', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Relation to prior sparse attention methods and design choices', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Training behavior and explored but discarded selection strategies', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Limitations and future directions', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)"
        ],
        "terms": [
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.95, provenance=[], term='sparse attention', definition='A way to speed up transformer models by computing attention scores for only a small, carefully chosen subset of tokens instead of every single one.', aliases=['sparsified attention', 'selective attention'], examples=['Sparse attention cuts the cost of summarising a 100-page report by skipping unrelated paragraphs.', 'Instead of looking at all 64 000 tokens, the model uses sparse attention to focus on just 2 500.'], sources=['b1', 'b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='arithmetic intensity', definition='The ratio of maths operations (like multiplications) to memory reads; high intensity means the GPU is kept busy computing rather than waiting for data.', aliases=['compute-to-memory ratio', 'FLOP/byte ratio'], examples=['Rewriting the kernel raised arithmetic intensity so the GPU finished 30 % faster.', 'When intensity is too low the task becomes memory-bound and extra cores don\u2019t help.'], sources=['b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.92, provenance=[], term='token compression', definition='Grouping neighbouring tokens into a single summary representation so the attention layer has fewer items to process.', aliases=['block compression', 'chunk summarisation'], examples=['Token compression turns each 32-token block into one vector, shrinking the key list by 32\u00d7.', 'After compression the model still sees the gist of the paragraph without reading every word.'], sources=['b3', 'b4'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.88, provenance=[], term='blockwise selection', definition='Choosing entire continuous chunks of tokens for attention instead of scattering individual positions, keeping memory access patterns fast on GPUs.', aliases=['chunk selection', 'block-based sparsity'], examples=['Blockwise selection picks the 16 most relevant sentence blocks instead of 500 separate words.', 'Because GPUs like contiguous data, blockwise selection gives a bigger real-world speed-up than random sparsity.'], sources=['b4', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.93, provenance=[], term='KV-cache', definition='The stored list of Key and Value vectors from earlier tokens that the model re-uses when generating each new word, avoiding recalculation.', aliases=['key-value store', 'past-context cache'], examples=['A 64 k-token KV-cache can occupy several gigabytes, so methods try to evict unneeded entries.', 'During decoding the GPU loads only the slimmed-down KV-cache instead of recomputing everything.'], sources=['b2', 'b4', 'b8'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='natively trainable', definition='Designed so that the sparse pattern is learned jointly with the rest of the model during training, not bolted on later.', aliases=['training-aware', 'end-to-end trainable'], examples=['Because NSA is natively trainable, gradients flow through the token-selection step.', 'Unlike post-hoc pruning, a natively trainable sparse layer keeps improving as the model sees more data.'], sources=['b1', 'b2', 'b12'])"
        ]
      },
      "timestamp": "2025-12-10T04:53:50.511024Z",
      "revision": 7
    },
    {
      "note": "step 'citizen' finished",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ],
        "explanations": [
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.', rationale='The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.', evidence=[EvidenceItem(text='NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.', rationale='Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.', evidence=[EvidenceItem(text='The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.', rationale='The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.', evidence=[EvidenceItem(text='Problem motivation and limitations of existing sparse attention', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA\u2019s overall framework and three-branch sparse attention design', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Detailed algorithmic components: compression, selection, sliding window, and gating', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Experimental setup and backbone architecture', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on general benchmarks', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on long-context benchmarks and retrieval', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Chain-of-thought reasoning results', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency measurements: training, prefill, and decoding', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Relation to prior sparse attention methods and design choices', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Training behavior and explored but discarded selection strategies', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Limitations and future directions', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)"
        ],
        "terms": [
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.95, provenance=[], term='sparse attention', definition='A way to speed up transformer models by computing attention scores for only a small, carefully chosen subset of tokens instead of every single one.', aliases=['sparsified attention', 'selective attention'], examples=['Sparse attention cuts the cost of summarising a 100-page report by skipping unrelated paragraphs.', 'Instead of looking at all 64 000 tokens, the model uses sparse attention to focus on just 2 500.'], sources=['b1', 'b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='arithmetic intensity', definition='The ratio of maths operations (like multiplications) to memory reads; high intensity means the GPU is kept busy computing rather than waiting for data.', aliases=['compute-to-memory ratio', 'FLOP/byte ratio'], examples=['Rewriting the kernel raised arithmetic intensity so the GPU finished 30 % faster.', 'When intensity is too low the task becomes memory-bound and extra cores don\u2019t help.'], sources=['b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.92, provenance=[], term='token compression', definition='Grouping neighbouring tokens into a single summary representation so the attention layer has fewer items to process.', aliases=['block compression', 'chunk summarisation'], examples=['Token compression turns each 32-token block into one vector, shrinking the key list by 32\u00d7.', 'After compression the model still sees the gist of the paragraph without reading every word.'], sources=['b3', 'b4'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.88, provenance=[], term='blockwise selection', definition='Choosing entire continuous chunks of tokens for attention instead of scattering individual positions, keeping memory access patterns fast on GPUs.', aliases=['chunk selection', 'block-based sparsity'], examples=['Blockwise selection picks the 16 most relevant sentence blocks instead of 500 separate words.', 'Because GPUs like contiguous data, blockwise selection gives a bigger real-world speed-up than random sparsity.'], sources=['b4', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.93, provenance=[], term='KV-cache', definition='The stored list of Key and Value vectors from earlier tokens that the model re-uses when generating each new word, avoiding recalculation.', aliases=['key-value store', 'past-context cache'], examples=['A 64 k-token KV-cache can occupy several gigabytes, so methods try to evict unneeded entries.', 'During decoding the GPU loads only the slimmed-down KV-cache instead of recomputing everything.'], sources=['b2', 'b4', 'b8'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='natively trainable', definition='Designed so that the sparse pattern is learned jointly with the rest of the model during training, not bolted on later.', aliases=['training-aware', 'end-to-end trainable'], examples=['Because NSA is natively trainable, gradients flow through the token-selection step.', 'Unlike post-hoc pruning, a natively trainable sparse layer keeps improving as the model sees more data.'], sources=['b1', 'b2', 'b12'])"
        ],
        "relevance_notes": [
          "RelevanceNote(kind='relevance.v1', version='1.0.0', confidence=0.0, provenance=[], target='System Message', rationale='Relevance analysis unavailable: LLM returned no usable JSON content.', score=0.0)"
        ]
      },
      "timestamp": "2025-12-10T04:54:18.125339Z",
      "revision": 8
    },
    {
      "note": "step 'review' finished",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ],
        "explanations": [
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.', rationale='The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.', evidence=[EvidenceItem(text='NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.', rationale='Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.', evidence=[EvidenceItem(text='The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.', rationale='The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.', evidence=[EvidenceItem(text='Problem motivation and limitations of existing sparse attention', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA\u2019s overall framework and three-branch sparse attention design', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Detailed algorithmic components: compression, selection, sliding window, and gating', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Experimental setup and backbone architecture', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on general benchmarks', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on long-context benchmarks and retrieval', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Chain-of-thought reasoning results', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency measurements: training, prefill, and decoding', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Relation to prior sparse attention methods and design choices', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Training behavior and explored but discarded selection strategies', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Limitations and future directions', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)"
        ],
        "terms": [
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.95, provenance=[], term='sparse attention', definition='A way to speed up transformer models by computing attention scores for only a small, carefully chosen subset of tokens instead of every single one.', aliases=['sparsified attention', 'selective attention'], examples=['Sparse attention cuts the cost of summarising a 100-page report by skipping unrelated paragraphs.', 'Instead of looking at all 64 000 tokens, the model uses sparse attention to focus on just 2 500.'], sources=['b1', 'b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='arithmetic intensity', definition='The ratio of maths operations (like multiplications) to memory reads; high intensity means the GPU is kept busy computing rather than waiting for data.', aliases=['compute-to-memory ratio', 'FLOP/byte ratio'], examples=['Rewriting the kernel raised arithmetic intensity so the GPU finished 30 % faster.', 'When intensity is too low the task becomes memory-bound and extra cores don\u2019t help.'], sources=['b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.92, provenance=[], term='token compression', definition='Grouping neighbouring tokens into a single summary representation so the attention layer has fewer items to process.', aliases=['block compression', 'chunk summarisation'], examples=['Token compression turns each 32-token block into one vector, shrinking the key list by 32\u00d7.', 'After compression the model still sees the gist of the paragraph without reading every word.'], sources=['b3', 'b4'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.88, provenance=[], term='blockwise selection', definition='Choosing entire continuous chunks of tokens for attention instead of scattering individual positions, keeping memory access patterns fast on GPUs.', aliases=['chunk selection', 'block-based sparsity'], examples=['Blockwise selection picks the 16 most relevant sentence blocks instead of 500 separate words.', 'Because GPUs like contiguous data, blockwise selection gives a bigger real-world speed-up than random sparsity.'], sources=['b4', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.93, provenance=[], term='KV-cache', definition='The stored list of Key and Value vectors from earlier tokens that the model re-uses when generating each new word, avoiding recalculation.', aliases=['key-value store', 'past-context cache'], examples=['A 64 k-token KV-cache can occupy several gigabytes, so methods try to evict unneeded entries.', 'During decoding the GPU loads only the slimmed-down KV-cache instead of recomputing everything.'], sources=['b2', 'b4', 'b8'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='natively trainable', definition='Designed so that the sparse pattern is learned jointly with the rest of the model during training, not bolted on later.', aliases=['training-aware', 'end-to-end trainable'], examples=['Because NSA is natively trainable, gradients flow through the token-selection step.', 'Unlike post-hoc pruning, a natively trainable sparse layer keeps improving as the model sees more data.'], sources=['b1', 'b2', 'b12'])"
        ],
        "relevance_notes": [
          "RelevanceNote(kind='relevance.v1', version='1.0.0', confidence=0.0, provenance=[], target='System Message', rationale='Relevance analysis unavailable: LLM returned no usable JSON content.', score=0.0)"
        ],
        "review_report": "ReviewReport(kind='review_report.v1', version='1.0.0', confidence=0.7, provenance=[], overall=0.8319400601173814, criteria=ReviewCriteria(kind='review_criteria.v1', version='1.0.0', confidence=0.7, provenance=[], accuracy=0.7, clarity=0.7277602404695259, completeness=1.0, safety=0.9), comments=['[readability] Aggregate readability score: 0.73 in [0,1].'], actions=[])"
      },
      "timestamp": "2025-12-10T04:54:18.127339Z",
      "revision": 9
    },
    {
      "note": "step 'brief' finished",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ],
        "explanations": [
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.', rationale='The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.', evidence=[EvidenceItem(text='NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.', rationale='Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.', evidence=[EvidenceItem(text='The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.', rationale='The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.', evidence=[EvidenceItem(text='Problem motivation and limitations of existing sparse attention', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA\u2019s overall framework and three-branch sparse attention design', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Detailed algorithmic components: compression, selection, sliding window, and gating', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Experimental setup and backbone architecture', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on general benchmarks', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on long-context benchmarks and retrieval', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Chain-of-thought reasoning results', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency measurements: training, prefill, and decoding', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Relation to prior sparse attention methods and design choices', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Training behavior and explored but discarded selection strategies', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Limitations and future directions', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)"
        ],
        "terms": [
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.95, provenance=[], term='sparse attention', definition='A way to speed up transformer models by computing attention scores for only a small, carefully chosen subset of tokens instead of every single one.', aliases=['sparsified attention', 'selective attention'], examples=['Sparse attention cuts the cost of summarising a 100-page report by skipping unrelated paragraphs.', 'Instead of looking at all 64 000 tokens, the model uses sparse attention to focus on just 2 500.'], sources=['b1', 'b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='arithmetic intensity', definition='The ratio of maths operations (like multiplications) to memory reads; high intensity means the GPU is kept busy computing rather than waiting for data.', aliases=['compute-to-memory ratio', 'FLOP/byte ratio'], examples=['Rewriting the kernel raised arithmetic intensity so the GPU finished 30 % faster.', 'When intensity is too low the task becomes memory-bound and extra cores don\u2019t help.'], sources=['b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.92, provenance=[], term='token compression', definition='Grouping neighbouring tokens into a single summary representation so the attention layer has fewer items to process.', aliases=['block compression', 'chunk summarisation'], examples=['Token compression turns each 32-token block into one vector, shrinking the key list by 32\u00d7.', 'After compression the model still sees the gist of the paragraph without reading every word.'], sources=['b3', 'b4'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.88, provenance=[], term='blockwise selection', definition='Choosing entire continuous chunks of tokens for attention instead of scattering individual positions, keeping memory access patterns fast on GPUs.', aliases=['chunk selection', 'block-based sparsity'], examples=['Blockwise selection picks the 16 most relevant sentence blocks instead of 500 separate words.', 'Because GPUs like contiguous data, blockwise selection gives a bigger real-world speed-up than random sparsity.'], sources=['b4', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.93, provenance=[], term='KV-cache', definition='The stored list of Key and Value vectors from earlier tokens that the model re-uses when generating each new word, avoiding recalculation.', aliases=['key-value store', 'past-context cache'], examples=['A 64 k-token KV-cache can occupy several gigabytes, so methods try to evict unneeded entries.', 'During decoding the GPU loads only the slimmed-down KV-cache instead of recomputing everything.'], sources=['b2', 'b4', 'b8'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='natively trainable', definition='Designed so that the sparse pattern is learned jointly with the rest of the model during training, not bolted on later.', aliases=['training-aware', 'end-to-end trainable'], examples=['Because NSA is natively trainable, gradients flow through the token-selection step.', 'Unlike post-hoc pruning, a natively trainable sparse layer keeps improving as the model sees more data.'], sources=['b1', 'b2', 'b12'])"
        ],
        "relevance_notes": [
          "RelevanceNote(kind='relevance.v1', version='1.0.0', confidence=0.0, provenance=[], target='System Message', rationale='Relevance analysis unavailable: LLM returned no usable JSON content.', score=0.0)"
        ],
        "review_report": "ReviewReport(kind='review_report.v1', version='1.0.0', confidence=0.7, provenance=[], overall=0.8319400601173814, criteria=ReviewCriteria(kind='review_criteria.v1', version='1.0.0', confidence=0.7, provenance=[], accuracy=0.7, clarity=0.7277602404695259, completeness=1.0, safety=0.9), comments=['[readability] Aggregate readability score: 0.73 in [0,1].'], actions=[])",
        "public_brief_md_path": "artifacts\\reports\\latest.md"
      },
      "timestamp": "2025-12-10T04:54:28.983971Z",
      "revision": 10
    },
    {
      "note": "planner: report written",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ],
        "explanations": [
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.', rationale='The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.', evidence=[EvidenceItem(text='NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.', rationale='Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.', evidence=[EvidenceItem(text='The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.', rationale='The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.', evidence=[EvidenceItem(text='Problem motivation and limitations of existing sparse attention', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA\u2019s overall framework and three-branch sparse attention design', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Detailed algorithmic components: compression, selection, sliding window, and gating', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Experimental setup and backbone architecture', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on general benchmarks', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on long-context benchmarks and retrieval', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Chain-of-thought reasoning results', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency measurements: training, prefill, and decoding', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Relation to prior sparse attention methods and design choices', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Training behavior and explored but discarded selection strategies', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Limitations and future directions', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)"
        ],
        "terms": [
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.95, provenance=[], term='sparse attention', definition='A way to speed up transformer models by computing attention scores for only a small, carefully chosen subset of tokens instead of every single one.', aliases=['sparsified attention', 'selective attention'], examples=['Sparse attention cuts the cost of summarising a 100-page report by skipping unrelated paragraphs.', 'Instead of looking at all 64 000 tokens, the model uses sparse attention to focus on just 2 500.'], sources=['b1', 'b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='arithmetic intensity', definition='The ratio of maths operations (like multiplications) to memory reads; high intensity means the GPU is kept busy computing rather than waiting for data.', aliases=['compute-to-memory ratio', 'FLOP/byte ratio'], examples=['Rewriting the kernel raised arithmetic intensity so the GPU finished 30 % faster.', 'When intensity is too low the task becomes memory-bound and extra cores don\u2019t help.'], sources=['b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.92, provenance=[], term='token compression', definition='Grouping neighbouring tokens into a single summary representation so the attention layer has fewer items to process.', aliases=['block compression', 'chunk summarisation'], examples=['Token compression turns each 32-token block into one vector, shrinking the key list by 32\u00d7.', 'After compression the model still sees the gist of the paragraph without reading every word.'], sources=['b3', 'b4'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.88, provenance=[], term='blockwise selection', definition='Choosing entire continuous chunks of tokens for attention instead of scattering individual positions, keeping memory access patterns fast on GPUs.', aliases=['chunk selection', 'block-based sparsity'], examples=['Blockwise selection picks the 16 most relevant sentence blocks instead of 500 separate words.', 'Because GPUs like contiguous data, blockwise selection gives a bigger real-world speed-up than random sparsity.'], sources=['b4', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.93, provenance=[], term='KV-cache', definition='The stored list of Key and Value vectors from earlier tokens that the model re-uses when generating each new word, avoiding recalculation.', aliases=['key-value store', 'past-context cache'], examples=['A 64 k-token KV-cache can occupy several gigabytes, so methods try to evict unneeded entries.', 'During decoding the GPU loads only the slimmed-down KV-cache instead of recomputing everything.'], sources=['b2', 'b4', 'b8'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='natively trainable', definition='Designed so that the sparse pattern is learned jointly with the rest of the model during training, not bolted on later.', aliases=['training-aware', 'end-to-end trainable'], examples=['Because NSA is natively trainable, gradients flow through the token-selection step.', 'Unlike post-hoc pruning, a natively trainable sparse layer keeps improving as the model sees more data.'], sources=['b1', 'b2', 'b12'])"
        ],
        "relevance_notes": [
          "RelevanceNote(kind='relevance.v1', version='1.0.0', confidence=0.0, provenance=[], target='System Message', rationale='Relevance analysis unavailable: LLM returned no usable JSON content.', score=0.0)"
        ],
        "review_report": "ReviewReport(kind='review_report.v1', version='1.0.0', confidence=0.7, provenance=[], overall=0.8319400601173814, criteria=ReviewCriteria(kind='review_criteria.v1', version='1.0.0', confidence=0.7, provenance=[], accuracy=0.7, clarity=0.7277602404695259, completeness=1.0, safety=0.9), comments=['[readability] Aggregate readability score: 0.73 in [0,1].'], actions=[])",
        "public_brief_md_path": "artifacts\\reports\\latest.md",
        "planner_plan_spec.replan": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": ""
        },
        "planner_report": {
          "strategy": "llm_planner.v1",
          "enable_history": false,
          "initial_steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "replan_steps": null,
          "refine_used": false,
          "replan_reason": null,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested."
        }
      },
      "timestamp": "2025-12-10T04:54:32.145236Z",
      "revision": 12
    },
    {
      "note": "pipeline: complete",
      "data": {
        "input_data": "samples\\Native_Sparse_Attention.pdf",
        "planner_plan_spec.initial": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": null
        },
        "planner_dag": {
          "strategy": "llm_planner.v1",
          "nodes": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "edges": {
            "parse": [
              "translate"
            ],
            "translate": [
              "jargon"
            ],
            "jargon": [
              "citizen"
            ],
            "citizen": [
              "review"
            ],
            "review": [
              "brief"
            ],
            "brief": []
          },
          "topo": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ]
        },
        "parsed_chunks": [
          {
            "id": "b1",
            "type": "paragraph",
            "page": 1,
            "text": "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.\n1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab. 2DeepSeek-AI.3UniversityofWashington.\nCorrespondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges. Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities. We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling. NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision. Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware. (2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance. As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning. Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.\n1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).\nRecentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation. Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.\nmodels,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies. However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases. Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.\nAnaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance. Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention: Hardware-Aligned and Natively Trainable\nSparse Attention\nJingyangYuan1,*,HuazuoGao2,DamaiDai2,JunyuLuo1,LiangZhao2,\nZhengyanZhang2,ZhendaXie2,Y.X.Wei2,LeanWang2,ZhipingXiao3,\nYuqingWang2,ChongRuan2,MingZhang1,\u2020,WenfengLiang2,\u2020,WangdingZeng2,\u2020\n*ContributionduringinternshipatDeepSeek-AI.\u2020Correspondingauthors.",
              "1KeyLaboratoryforMultimediaInformationProcessing,SchoolofComputerScience,\nPekingUniversity,PKU-AnkerLLMLab.",
              "2DeepSeek-AI.3UniversityofWashington.",
              "Correspondence:yuanjy@pku.edu.cn,mzhang_cs@pku.edu.cn,wenfeng.liang@deepseek.com,zengwangding@deepseek.com\nAbstract\nLong-context modeling is crucial for next-\ngenerationlanguagemodels,yetthehighcom- 0.5\nputational cost of standard attention mecha-\n0.4\nnisms poses significant computational chal-\nlenges.",
              "Sparse attention offers a promising 0.3\ndirectionforimprovingefficiencywhilemain- 0.2\ntaining model capabilities.",
              "We present NSA,\n0.1\na Natively trainable Sparse Attention mecha-\nnism that integrates algorithmic innovations 0.0\nGeneral LongBenchReasoning\nwithhardware-alignedoptimizationstoachieve\nefficientlong-contextmodeling.",
              "NSAemploys\nadynamichierarchicalsparsestrategy,combin-\ningcoarse-grainedtokencompressionwithfine-\ngrainedtokenselectiontopreservebothglobal\ncontextawarenessandlocalprecision.",
              "Ourap-\nproachadvancessparseattentiondesignwith\ntwokeyinnovations:(1)Weachievesubstantial\nspeedupsthrougharithmeticintensity-balanced\nalgorithm design, with implementation opti-\nmizations for modern hardware.",
              "(2) We en-\nableend-to-endtraining,reducingpretraining\ncomputationwithoutsacrificingmodelperfor-\nmance.",
              "As shown in Figure 1, experiments\nshow the model pretrained with NSA main-\ntainsorexceedsFullAttentionmodelsacross\ngeneral benchmarks, long-context tasks, and\ninstruction-basedreasoning.",
              "Meanwhile,NSA\nachievessubstantialspeedupsoverFullAtten-\ntionon64k-lengthsequencesacrossdecoding,\nforward propagation, and backward propaga-\ntion, validating its efficiency throughout the\nmodellifecycle.",
              "1 Introduction\nTheresearchcommunityincreasinglyrecognizes\nlong-context modeling as a crucial capability for\nnext-generation large language models, driven\nby diverse real-world applications ranging from\nin-depth reasoning (DeepSeek-AI, 2025; Zelik-\nman et al., 2022), repository-level code genera-\ntion(Zhangetal.,2023a;Zhangetal.) andmulti-\nturnautonomousagentsystems(Parketal.,2023).",
              "Recentbreakthroughs,includingOpenAI\u2019so-series\nProceedingsofthe63rdAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages23078\u201323097\nJuly27-August1,2025\u00a92025AssociationforComputationalLinguistics\nerocS\nPerformanceonBenchmarks\n13.0\nFullAttention\nNSA 11.0\n9.0\n7.0\n5.0\n3.0\n1.0\nDecode Forward Backward\noitaRpudeepS\nSpeedonStages\n11.6\u00d7\n9.0\u00d7\n6.0\u00d7\nFigure1: Comparisonofperformanceandefficiencybe-\ntweenFullAttentionmodelandourNSA.Left: Despite\nbeing sparse, NSA surpasses Full Attention baseline\non average across general benchmarks, long-context\ntasks,andreasoningevaluation.",
              "Right: For64k-length\nsequence processing, NSA achieves substantial com-\nputational speedup compared to Full Attention in all\nstages: decoding,forward,andbackwardpropagation.",
              "models,DeepSeek-R1(DeepSeek-AI,2025),and\nGemini 1.5 Pro (Google et al., 2024), enabling\nmodelstoprocessentirecodebases,lengthydocu-\nments,maintaincoherentmulti-turnconversations\noverthousandsoftokens,andperformcomplexrea-\nsoningacrosslong-rangedependencies.",
              "However,\nthehighcomplexity (Zaheeretal.,2020)ofAtten-\ntion (Vaswani et al., 2017) mechanisms emerges\nasacriticallatencybottleneckassequencelength\nincreases.",
              "Theoretical estimates indicate that at-\ntentioncomputationwithsoftmaxarchitecturesac-\ncountsfor70\u201380%oftotallatencywhendecoding\n64k-lengthcontexts,underscoringtheurgentneed\nformoreefficientattentionmechanisms.",
              "Anaturalapproachtoefficientlong-contextmod-\nelingistotakeadvantageoftheinherentsparsity\nofsoftmaxattention(Geetal.,2023;Jiangetal.,\n2023),whereselectivelycomputingcriticalquery-\nkey pairs can significantly reduce computational\noverheadwhilepreservingperformance.",
              "Recentad-\nvancesdemonstratethispotentialthroughdiverse\n23078"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b2",
            "type": "paragraph",
            "page": 2,
            "text": "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion. WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024). More related sparse attention meth- plementations. Experimentalresultsdemonstrate\nods are introduced in Appendix A. Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments. Many ing existing sparse attention approaches. Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention. increasingforlongersequences. Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.\nConvertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization. Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology. Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.\ncostswhilemaintainingmodelperformance. These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.\ntoachievefastlong-contextinferenceortraining.\nWhenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.\nAttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v . Formally,\n:t\nerarchical token modeling. As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information. Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a . li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .\nP j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.\nt i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity. (2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators. Thisoptimization lengesforlong-contextprocessing.\nenablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining. operations to memory accesses. It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware. Each\nperiments on real-world language corpora. Pre- GPUhasacriticalarithmeticintensitydetermined\n23079",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "strategies: KV-cache eviction methods (Zhang trainingona27B-parametertransformerbackbone\net al., 2023b; Li et al., 2024; Zhou et al., 2024), with260Btokens,weassessNSA\u2019sperformance\nblockwiseKV-cacheselectionmethods(Tangetal., acrossgenerallanguageevaluations,long-context\n2024;Xiaoetal.,2024a;Gaoetal.,2024),andsam- evaluations,andchain-of-thoughtreasoningevalua-\npling,clusteringorhashing-basedselectionmeth- tion.",
              "WefurthercomparethekernelspeedonA100\nods (Chen et al., 2024b; Liu et al., 2024; Desai GPUswithoptimizedTriton(Tilletetal.,2019)im-\net al., 2024).",
              "More related sparse attention meth- plementations.",
              "Experimentalresultsdemonstrate\nods are introduced in Appendix A.",
              "Despite these thatNSAachievescomparableorsuperiorperfor-\npromisingstrategies,existingsparseattentionmeth- mancetoFullFttentionbaseline,whileoutperform-\nodsoftenfallshortinpracticaldeployments.",
              "Many ing existing sparse attention approaches.",
              "Addi-\napproaches fail to achieve speedups comparable tionally,NSAdeliverssubstantialspeedupsacross\ntotheirtheoreticalgains;moreover,mostmethods decoding,forward,andbackwardstagescompared\nlackeffectivetraining-timesupporttofullyexploit to vanilla Full Attention, with the speedup ratio\nthesparsitypatternsofattention.",
              "increasingforlongersequences.",
              "Theseresultsval-\nToaddresstheselimitations,thedeploymentof idatethatourhierarchicalsparseattentiondesign\neffectivesparseattentionmusttackletwokeychal- effectivelybalancesmodelcapabilityandcomputa-\nlenges: (1)Hardware-alignedinferencespeedup: tionalefficiency.",
              "Convertingtheoreticalcomputationreductionsinto\nactual speed improvements requires hardware- 2 Methodology\nfriendly algorithm design during both prefilling\nOurtechnicalapproachspansalgorithmdesignand\nand decoding stages to mitigate memory access\nkerneloptimization.",
              "Inthefollowingsubsections,\nandhardwareschedulingbottlenecks;(2)Training-\nwefirstintroducethebackgroundofourmethod-\nawarealgorithmdesign: Enablingend-to-endcom-\nology.",
              "Thenwepresenttheoverallframeworkof\nputationwithtrainableoperatorstoreducetraining\nNSA,followedbyitskeyalgorithmiccomponents.",
              "costswhilemaintainingmodelperformance.",
              "These\nFinally, wedetailourhardware-optimizedkernel\nrequirementsarecrucialforreal-worldapplications\ndesignthatmaximizespracticalefficiency.",
              "toachievefastlong-contextinferenceortraining.",
              "Whenconsideringbothaspects,existingmethods 2.1 Background\nstillexhibitanoticeablegap.",
              "AttentionMechanismiswidelyusedinlanguage\nTo achieve more effective and efficient sparse\nmodeling where each query token q computes\nt\nattention, we present NSA, a Natively trainable\nrelevancescoresagainstallprecedingkeysk to\n:t\nSparse Attention architecture that integrates hi-\ngenerate aweighted sum of values v .",
              "Formally,\n:t\nerarchical token modeling.",
              "As shown in Fig-\nfor an input sequence of length t, the attention\nure 2, NSA reduces per-query computation by\noperationisdefinedas:\norganizing keys and values into temporal blocks\nandprocessingthemthroughthreeattentionpaths: o = Attn(q ,k ,v ) (1)\nt t :t :t\ncompressedcoarse-grainedtokens,selectivelyre-\ntained fine-grained tokens, and sliding windows whereAttndenotestheattentionfunction:\nfor local contextual information.",
              "Then we im-\np ti l c e a m le e f n fi t c s i p e e n c c i y a .",
              "li N ze S d A k i e n r t n ro el du to ce m s a tw xi o m c i o ze re it i s nn p o r v ac a - - Attn(q t ,k :t ,v :t ) = t i= t 1 \u03b1 \u03b1 t,i v i ,\u03b1 t,i = e q \u221a \u22a4t d k k i .",
              "P j=1 t,j\ntionscorrespondingtothekeyrequirementsabove: (2)\nP\n(1) Hardware-aligned system speedup: Optimize Here,\u03b1 representstheattentionweightbetween\nt,i\nblockwisesparseattentionforTensorCoreutiliza- q andk ,andd isthefeaturedimensionofkeys.",
              "t i k\ntionandmemoryaccess,ensuringbalancedarith- Assequencelengthincreases, attentioncomputa-\nmeticintensity.",
              "(2)Training-awaredesign: Enable tion becomes increasingly dominant in the over-\nstable end-to-end training through efficient algo- allcomputationalcost,presentingsignificantchal-\nrithmsandbackwardoperators.",
              "Thisoptimization lengesforlong-contextprocessing.",
              "enablesNSAtosupportbothefficientdeployment Arithmetic Intensity is the ratio of compute\nandend-to-endtraining.",
              "operations to memory accesses.",
              "It intrinsically\nWe evaluate NSA through comprehensive ex- shapesalgorithmoptimizationonhardware.",
              "Each\nperiments on real-world language corpora.",
              "Pre- GPUhasacriticalarithmeticintensitydetermined\n23079"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b3",
            "type": "paragraph",
            "page": 3,
            "text": "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...\nCompression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture. Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext. Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch. Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.\nbyitspeakcomputecapabilityandmemoryband- k ,v . Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits. For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c). (5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth). AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values. gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators. In con- viaanMLPandsigmoidactivation. LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc]. (6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity. Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.\n\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.\nInthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.\nToleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t . pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock. Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Native Sparse Attention Mechanism\nSplit to Continuous Blocks\nCompressed Attention Mask\nCompress Top-n Selection Selection ...",
              "Compression\nConcat Sliding Selected Attention Mask\nCompressed Attention Selected Attention Sliding Attention\nSliding Attention Mask\nOutput Output Output\nGated Output Attention Score Query Token Activated Token Evicted Token Ignored Token\nFigure2: OverviewofNSA\u2019sarchitecture.",
              "Left: Theframeworkprocessesinputsequencesthroughthreeparallel\nattention branches: For a given query, preceding keys and values are processed into compressed attention for\ncoarse-grainedpatterns,selectedattentionforimportanttokenblocks,andslidingattentionforlocalcontext.",
              "Right:\nVisualizationofdifferentattentionpatternsproducedbyeachbranch.",
              "Greenareasindicateregionswhereattention\nscoresneedtobecomputed,whilewhiteareasrepresentregionsthatcanbeskipped.",
              "byitspeakcomputecapabilityandmemoryband- k ,v .",
              "Wecandesignvariousmappingstrategies\n:t :t\nwidth,calculatedastheratioofthesetwohardware\ntogetdifferentcategoriesofK\u02dcc,V\u02dcc,andcombine\nt t\nlimits.",
              "For computation tasks, arithmetic inten- themasfollows:\nsityabovethiscriticalthresholdbecomescompute-\nbound (limited by GPU FLOPS), while below it o \u2217t = g t c \u00b7 Attn(q t ,K\u02dc t c,V\u02dc t c).",
              "(5)\nbecomesmemory-bound(limitedbymemoryband- c\nX\u2208C\nwidth).",
              "AsillustratedinFigure2,NSAhavethreemapping\nSpecificallyforcausalself-attentionmechanism, strategies = cmp,slc,win ,representingcom-\nC { }\nduringtrainingandprefillingphases,batchedma- pression, selection, and sliding window for keys\ntrixmultiplicationsandattentioncomputationsex- and values.",
              "gc [0,1] is the gate score for cor-\nt \u2208\nhibithigharithmeticintensity,makingthesestages respondingstrategyc,derivedfrominputfeatures\ncompute-bound on modern accelerators.",
              "In con- viaanMLPandsigmoidactivation.",
              "LetN denote\nt\ntrast,auto-regressivedecodingbecomesmemory- thetotalnumberofremappedkeys/values:\nbandwidth constrained because it generates one\ntokenperforwardpasswhilerequiringloadingthe N = size[K\u02dcc].",
              "(6)\nt t\nentirekey-valuecache,resultinginlowarithmetic c\nX\u2208C\nintensity.",
              "Thisleadstodifferentoptimizationgoals\nWemaintainahighsparsitybyensuringN t.",
              "\u2014reducingcomputationcostduringtrainingand t \u226a\nprefilling, while reducing memory access during\n2.3 AlgorithmDesign\ndecoding.",
              "Inthissubsection,weintroducethedesignofour\nremapping strategies f and f : token compres-\n2.2 OverallFramework K V\nsion,tokenselection,andslidingwindow.",
              "Toleveragethepotentialofattentionwithnatural\nsparse pattern, we propose replacing the original 2.3.1 TokenCompression\nkey-valuepairsk :t ,v :t inEquation1withamore Byaggregatingsequentialblocksofkeysorvalues\ncompactandinformation-densesetofrepresenta- into block-level representations, we obtain com-\ntion key-value pairs K\u02dc t ,V\u02dc t given each query q t .",
              "pressedkeysandvaluesthatcapturetheinforma-\nSpecifically,weformallydefinetheoptimizedat- tionoftheentireblock.",
              "Formally,thecompressed\ntentionoutputasfollows: keyrepresentationK\u02dccmp iscalculatedby:\nt\nK\u02dc = f (q ,k ,v ),V\u02dc = f (q ,k ,v ) (3) t l\nt K t :t :t t V t :t :t cmp\nf (k ) = \u03d5(k ) 1 i \u2212\nK :t id+1:id+l \u2264 \u2264 d\n(cid:26) (cid:12) (cid:22) (cid:23)(cid:27)\no \u2217t = Attn q t ,K\u02dc t ,V\u02dc t (4) (cid:12) (cid:12) (7)\nwherel istheblocklength, d(cid:12)istheslidingstride\n(cid:16) (cid:17)\nwhereK\u02dc ,V\u02dc aredynamicallyconstructedbased betweenadjacentblocks,and\u03d5isalearnableMLP\nt t\nonthecurrentqueryq andthecontextualmemory with intra-block position encoding to map keys\nt\n23080"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b4",
            "type": "paragraph",
            "page": 4,
            "text": "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r . es K i \u02dc on t cm k p ey \u2208 s. w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n . s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize. Whencompression\nfragmentation. Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp . Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly. For cases where\nt t\nputationalburdenofattention. theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship. Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values. Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead. \u00b7\ning vector element. For models employing GQA\nBlockwiseSelection. Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding. Thesharedimportancescores\npatternsofattentionscores. Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs. ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads. Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores. This archi-\nandH isthenumberofqueryheadsineachgroup.\ntectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.\nprincipleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection. Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign. Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores. Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels. OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.\nwhere rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks. To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.\nc\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n. B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys. Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.\ngrained value\nV\u02dcslc.\nThe selected keys and values\nt\nImportanceScoreComputation. Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.\nt\noverhead. Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "R in dk a \u00d7 b \u230a l t o \u2212d c l k \u230b i t s o te a n s s i o n r g c l o e m c p o o m s p ed re b s y se c d om ke p y r .",
              "es K i \u02dc on t cm k p ey \u2208 s.",
              "w tw h e e e r n e q p t c t m an p d \u2208 co R m \u230a p t \u2212d re l \u230b ss i i s on th k e ey at s te K\u02dc nt t c i m o p n .",
              "s L co e r t e l s \u2032 d b e e - -\nUsually, we adopt d < l to mitigate information notetheselectionblocksize.",
              "Whencompression\nfragmentation.",
              "Ananalogousformulationholdsfor blocksandselectionblockssharethesameblock-\nthecompressedvaluerepresentationV\u02dc t cmp .",
              "Com- ing scheme, i.e., l \u2032 = l = d, we can directly ob-\npressed representations capture coarser-grained taintheselectionblockimportancescorespslc by\nt\nhigher-levelsemanticinformationandreducecom- pslc = p cmp straightforwardly.",
              "For cases where\nt t\nputationalburdenofattention.",
              "theblockingschemesdiffer,wederivetheimpor-\ntancescoresforselectionblocksaccordingtotheir\n2.3.2 TokenSelection\nspatialrelationship.",
              "Givend landd l ,wehave:\n\u2032\n| |\nUsingonlycompressedkeys,valuesmightloseim-\nportantfine-grainedinformation,motivatingusto l\u2032 1 l 1\nd\u2212 d\u2212 l\nselectivelypreserveindividualkeys,values.",
              "Below pslc[j] = p cmp \u2032 j m n , (9)\nt t d \u2212 \u2212\nwe describe our efficient token selection mecha- m=0n=0 (cid:20) (cid:21)\nX X\nnismthatidentifiesandpreservesthemostrelevant\nwhere[ ]denotestheindexingoperatorforaccess-\ntokenswithlowcomputationaloverhead.",
              "\u00b7\ning vector element.",
              "For models employing GQA\nBlockwiseSelection.",
              "Ourselectionstrategypro-\norMQAwherekey-valuecachesaresharedacross\ncesseskeyandvaluesequencesinspacialcontinu-\nqueryheads,consistentblockselectionacrossthese\nousblocks,motivatedbytwokeyfactors: hardware\nheadshastobeensuredtominimizeKVcacheload-\nefficiencyconsiderationsandinherentdistribution\ningduringdecoding.",
              "Thesharedimportancescores\npatternsofattentionscores.",
              "Blockwiseselectionis\nacrossheadsinagroupareformallydefinedas:\ncrucialtoachieveefficientcomputationonmodern\nGPUs.",
              "ThatisbecausemodernGPUarchitectures\nH\nexhibitsignificantlyhigherthroughputforcontin- pslc \u2032 = p slc,(h) (10)\nt t\nuous block accesses compared to random index-\nh=1\nX\nbasedreads.",
              "Also,blockwisecomputationenables\nwhere(h)inthesuperscriptdenotestheheadindex,\noptimal utilization of Tensor Cores.",
              "This archi-\nandH isthenumberofqueryheadsineachgroup.",
              "tectural characteristic has established blockwise\nThisaggregationensuresconsistentblockselection\nmemoryaccessandcomputationasafundamental\nacrossheadswithinthesamegroup.",
              "principleinhigh-performanceattentionimplemen-\nTop-nnnBlockSelection.",
              "Afterobtainingthese-\ntations,asexemplifiedbyFlashAttention\u2019sblock-\nlectionblockimportancescores,Weretaintokens\nbaseddesign.",
              "Blockwiseselectionfollowstheinher-\nwithin the top-n sparse blocks ranked by block\nentdistributionpatternsofattentionscores.",
              "Prior\nimportancescores,formulatedas:\nworks(Jiangetal.,2024)haveshownthatattention\nscoresoftenexhibitspatialcontinuity,suggesting\nI t\n=\n{\ni\n|\nrank(ps\nt\nlc \u2032[i])\n\u2264\nn\n}\n(11)\nthatneighboringkeystendtosharesimilarimpor-\ntancelevels.",
              "OurvisualizationinAppendixFalso K\u02dc t slc = Cat { k il \u2032 +1:(i+1)l \u2032| i \u2208 I t } , (12)\nshowsthisspatialcontinuouspattern.",
              "where rank( ) de(cid:2)notes the ranking posi(cid:3)tion in de-\nTo implement blockwise selection, we first di- \u00b7\nscending order, with rank = 1 corresponding to\nvidekey,valuesequencesintoselectionblocks.",
              "To\nthe highest score, is the set of selected blocks\u2019\nt\nidentify the most important blocks for attention I\nindices,Catdenotestheconcatenationoperation.",
              "c\nto\nom\nea\np\nc\nu\nh\nta\nb\nti\nl\no\no\nn\nc\n,\nk\nw\n.",
              "B\ne\ne\nn\nl\ne\no\ne\nw\ndt\nw\no\ne\nas\np\ns\nr\ni\ne\ng\ns\nn\nen\nim\nt o\npo\nu\nr\nr\nta\nm\nn\ne\nc\nt\ne\nh\ns\no\nc\nd\no\nf\nr\no\nes\nr\nK\u02dc\nt\nslc\n\u2208\nR dk\u00d7 nl\u2032 istensorcomposedbycompresion\nkeys.",
              "Ananalogousformulationappliestothefine-\ncomputingtheseblock-levelimportancescores.",
              "grained value\nV\u02dcslc.",
              "The selected keys and values\nt\nImportanceScoreComputation.",
              "Computing\nthenparticipateintheattentioncomputationwith\nblockimportancescorescouldintroducesignificant\nq asdefinedinEquation5.",
              "t\noverhead.",
              "Fortunately, the attention computation\nofcompressiontokensproducesintermediateatten- 2.3.3 SlidingWindow\ntionscoresthatwecanleveragetoinduceselection In attention mechanisms, local patterns typically\nblockimportancescores,formulatedas: adapt faster and can dominate the learning pro-\ncess,potentiallypreventingthemodelfromeffec-\np c t mp = Softmax qT t K\u02dc t cmp (8) tivelylearningfromcompressionandselectionto-\n(cid:16) (cid:17) 23081"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b5",
            "type": "paragraph",
            "page": 5,
            "text": "kens. To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns. Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches. These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism. To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches. This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.\nAfterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5. Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.\n2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton. Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs. While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention. Figure3illustrates\nthe forward pass implementation. If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks. To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM. The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1. Group-CentricDataLoading. Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA. The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.\nnerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .\nt\nI\n2. SharedKVFetching. Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .\nk \u2032\n|\n3. Outer Loop on Grid. Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.\nThisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.\n3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods. We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "kens.",
              "To address this issue, we introduce a ded- K Inner Loop\nicated sliding window branch that explicitly han-\nQ\ndleslocalcontext,allowingotherbranches(com- Select In\npression and selection) to focus on learning their\nrespective features without being shortcutted by\nlocal patterns.",
              "Specifically, we maintain recent\ntokens K\u02dc t win = k t w:t ,V\u02dc t win = v t w:t in a win- Load\n\u2212 \u2212\ndoww, andisolateattentioncomputationsofdif- Select In\nferent information sources (compression tokens, Load Compute on SRAM\nandselectedtokens,slidingwindow)intoseparate\nbranches.",
              "These branch outputs are then aggre-\nOutput to HBM\ngated through a learned gating mechanism.",
              "To\nfurther prevent shortcut learning across attention Output\nbranches with marginal computational overhead,\nweprovideindependentkeysandvaluesforthree\nbranches.",
              "This architectural design enables sta-\nble learning by preventing gradient interference\nbetweenlocalandlong-rangepatternrecognition,\nwhileintroducingminimaloverhead.",
              "Afterobtainingallthreecategoriesofkeysand\nvalues\n(K\u02dccmp ,V\u02dccmp\n;\nK\u02dcslc,V\u02dcslc;\nand\nK\u02dcwin,V\u02dcwin),\nt t t t t t we compute the final attention output following\nEquation5.",
              "Togetherwiththecompression,selec-\ntion, and sliding window mechanisms described\nabove,thisformsthecompletealgorithmicframe-\nworkofNSA.",
              "2.4 KernelDesign\nTo achieve FlashAttention-level speedup during\nthetrainingandprefilling,weimplementhardware-\nalignedsparseattentionkernelsuponTriton.",
              "Given\nMHA is memory-intensive and inefficient for de-\ncoding,wefocusonarchitectureswithsharedKV\ncacheslikeGQAandMQAfollowingthecurrent\nstate-of-the-art LLMs.",
              "While compression and\nsliding window attention computations are read-\nilycompatiblewithexistingFlashAttention-2ker-\nnels, we introduce the specialized kernel design\nforsparseselectionattention.",
              "Figure3illustrates\nthe forward pass implementation.",
              "If we were to\nfollowFlashAttention\u2019sstrategyofloadingtempo-\nrallycontinuousqueryblocksintoSRAM,itwould\nresult in inefficient memory access since queries\nwithinablockmayrequiredisjointKVblocks.",
              "To\naddressthis,ourkeyoptimizationliesinadifferent\nquerygroupingstrategy: foreachpositiononthe\nquerysequence,weloadallqueryheadswithina\nGQAgroup(theysharethesamesparseKVblocks)\ninto SRAM.",
              "The proposed kernel architecture is\ncharacterizedbythefollowingkeyfeatures:\n1.",
              "Group-CentricDataLoading.",
              "Foreachin-\npooL\ndirG\nV\nInner\nLoop\nGrid Loop\nFigure 3: Kernel design for NSA.",
              "The kernel loads\nqueries by GQA groups (Grid Loop), fetches corre-\nspondingsparseKVblocks(InnerLoop),andperforms\nattentioncomputationonSRAM.Greenblocksindicate\ndataonSRAM,whileblueindicatesdataonHBM.",
              "nerloop,loadallheads\u2019queriesQ R [h,dk]\n\u2208\nin the group at position t and their shared\nsparsekey/valueblockindices .",
              "t\nI\n2.",
              "SharedKVFetching.",
              "Intheinnerloop,Se-\nquentially load continuous key/value blocks\nindexed by into SRAM as K\nt\nI \u2208\nR [Bk,dk],V R [Bk,dv] to minimize memory\n\u2208\nloading, where B is the kernel block size\nk\nsatisfyingB l .",
              "k \u2032\n|\n3.",
              "Outer Loop on Grid.",
              "Since the inner-loop\nlength (proportional to the selected block\ncountn)remainsnearlyidenticalfordifferent\nquery blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and opti-\nmizethekernel.",
              "Thisdesignachievesnear-optimalarithmeticin-\ntensityby(1)eliminatingredundantKVtransfers\nthroughgroup-wisesharing,and(2)balancingcom-\nputeworkloadsofGPUstreamingmultiprocessors.",
              "3 Experiments\nWeevaluateNSAthroughthreelenses: (1)general\nbenchmarksperformance,(2)long-contextbench-\nmarksperformance,and(3)chain-of-thoughtrea-\nsoningperformance,comparingagainstFullAtten-\ntion baseline and state-of-the-art sparse attention\nmethods.",
              "We defer the efficiency analysis of our\nsparsecomputationparadigmtoSection4,where\n23082"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b6",
            "type": "paragraph",
            "page": 6,
            "text": "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.\nAcc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks. NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.\nSQA MQA Synthetic Code\nModel Avg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories. NSAoutperformedmostofthebaselines\nincludingFullAttention.\nwe provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed. fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions. Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.\nof-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters. Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance. More equivalenttoFullAttention. Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting. Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512. Both comparison. Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.\nto achieve long-context adaptation. Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison. WeprovidethelosscurveinAppendixD. GeneralEvaluation. Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval\nModel Avg.",
              "Acc.5-shot Acc.5-shot Acc.5-shot Acc.3-shot Acc.8-shot Acc.4-shot F11-shot Pass@13-shot Pass@10-shot\nFullAttn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable1: PretrainingperformancecomparisonbetweenthefullattentionbaselineandNSAongeneralbenchmarks,\nacross knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding\n(MBPP,HumanEval)tasks.",
              "NSAachievessuperioraverageperformanceonmostbenchmarksdespitehighsparsity.",
              "SQA MQA Synthetic Code\nModel Avg.",
              "MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFullAttn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single\ndocumentQA,multi-documentQA,syntheticandcodetaskcategories.",
              "NSAoutperformedmostofthebaselines\nincludingFullAttention.",
              "we provides detailed discussions on training and etal.,2024),andExact-Top,whichfirstcomputes\ninferencespeed.",
              "fullattentionscoreandselectthetop-nscoreskeys\ncorresponding to each query and then calculates\n3.1 PretrainingSetup\nattentiononthesepositions.",
              "Thesemethodsspan\nFollowingthecommonpracticeinstate-of-the-art diversesparseattentionparadigms,includingKV-\nLLMs,ourexperimentsadoptabackbonecombin- cache eviction, query-aware selection, and exact\ningGrouped-QueryAttention(GQA)andMixture- top-nsparseselection.",
              "of-Experts(MoE),featuring27Btotalparameters Forgeneralevaluation,wheremostsampleshave\nwith3Bactiveparameters.",
              "Wechosethisarchitec- lengthswithinthelocalcontextwindowofsparse\nture as it achieves an effective trade-off between attention baselines, these methods are effectively\ncomputation cost and model performance.",
              "More equivalenttoFullAttention.",
              "Therefore,wepresent\ndetails about our framework are provided in Ap- only the comparison results between NSA and\npendixC.ForNSA,wesetcompressionblocksize FullAttentionbaselineinthissetting.",
              "Inthelong-\nl = 32, slidingstrided = 16, selectedblocksize contextevaluation,weconductcomparisonsacross\nl = 64, selected block count n = 16 (includ- allbaselinemethods,withthesparsityofallsparse\n\u2032\ning fixed activating the 1 inital block and 2 local attentionmethodssettothesametoensureafair\nblocks), andslidingwindowsizew = 512.",
              "Both comparison.",
              "Forchain-of-thoughtreasoningeval-\nFullAttentionandsparseattentionmodelsarepre- uation, which requires long-text supervised fine-\ntrainedon270Btokensof8k-lengthtexts,followed tuning, we also limit our comparison to Full At-\nby continued training and supervised fine-tuning tention, asmostsparseattentionbaselinesdonot\non32k-lengthtextswithYaRN(Pengetal.,2024) supporttraining.",
              "to achieve long-context adaptation.",
              "Both models\n3.3 PerformanceComparison\naretrainedtofullconvergencetoensurefaircom-\nparison.",
              "WeprovidethelosscurveinAppendixD.",
              "GeneralEvaluation.",
              "Weevaluatedthepretrained\nNSA and Full Attention baseline, on a com-\n3.2 BaselinesMethods\nprehensive suite of benchmarks spanning knowl-\nInadditiontocomparingwithFullAttentionbase- edge, reasoning, and coding capabilities, includ-\nline,weevaluateseveralstate-of-the-artinference- ing MMLU (Hendrycks et al., 2020), MMLU-\nstagesparseattentionmethods: H2O(Zhangetal., PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023b),infLLM(Xiaoetal.,2024a),Quest(Tang 2023),BBH(Suzgunetal.,2022),GSM8K(Cobbe\n23083"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b7",
            "type": "paragraph",
            "page": 7,
            "text": "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning. OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength. NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.\nTable4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding. Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.\n2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline. Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics. This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance. Notably,NSAdemonstratessignifi- quencelengths. FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens. We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms. Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons. As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways. Theconsistentperformanceacross and +0.046 over Exact-Top). This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture. sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation. As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test. Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.\nficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval. The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information. ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision. trieval(PassR-en: +0.075). Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "0\n11\n22\n33\n44\n55\n66\n77\n88\n100\n1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\nContextLength\n)%(tnecrePhtpeD\nNeedleinAHaystack27BNSA64kContext\nGenerationTokenLimit 8192 16384\n1.0\nFullAttention-R 0.046 0.092\n0.8\nNSA-R 0.121 0.146\n0.6 Table3: AIMEInstruction-basedEvaluatingaftersu-\npervisedfine-tuning.",
              "OurNSA-Rdemonstratesbetter\n0.4 performancethanFullAttention-Ratboth8kand16k\nsequencelengths\n0.2\n0.0 ContextLength 8192 16384 65536\nFullAttention 8192 16384 65536\nFigure 4: Needle-in-a-Haystack retrieval accuracy NSA 2048 2560 5632\nacrosscontextpositionswith64kcontextlength.",
              "NSA\nExpectedSpeedup 4 6.4 11.6\nachievesperfectaccuracythroughitshierarchicalsparse \u00d7 \u00d7 \u00d7\nattentiondesign.",
              "Table4: Memoryaccessvolume(inequivalentnumber\noftokens)perattentionoperationduringdecoding.",
              "Due\ntothelowarithmeticintensityandmemory-boundna-\net al., 2021), MATH (Hendrycks et al., 2020),\ntureofdecoding,theexpectedspeedupisapproximately\nDROP (Dua et al., 2019), MBPP (Austin et al.,\nlinearwiththevolumeofmemoryaccess.",
              "2021), and HumanEval (Chen et al., 2021).",
              "The\nresults are shown in Table 1.",
              "Despite its spar-\nsity, NSA achieves superior overall performance, odsandFullAttentionbaseline.",
              "Toensureconsis-\noutperforming all baselines including Full Atten- tent sparsity, we set the token activated by each\ntion on 7 out of 9 metrics.",
              "This indicates that al- query in all sparse attention baselines to 2560 to-\nthoughNSAmaynotfullyleverageitsefficiency kens, which corresponds to the average number\nadvantagesonshortersequences,itshowsstrong oftokensactivatedinNSAwhenhandling32kse-\nperformance.",
              "Notably,NSAdemonstratessignifi- quencelengths.",
              "FollowingStreamLLM(Xiaoetal.,\ncantgainsinreasoning-relatedbenchmarks(DROP: 2023),thistokenbudgetincludestheleading128\n+0.042,GSM8K:+0.034),suggestingthatourpre- tokens and 512 local tokens.",
              "We exclude certain\ntraininghelpsmodelstodevelopspecializedatten- subsets from LongBench due to their low scores\ntionmechanisms.",
              "Thissparseattentionpretraining across all models, which may not provide mean-\nmechanism force model to focus on the most im- ingful comparisons.",
              "As shown in Table 2, NSA\nportantinformation,potentiallyenhancingperfor- achieves the highest average score 0.469, outper-\nmancebyfilteringoutnoisefromirrelevantatten- formingallbaselines(+0.032overFullAttention\ntionpathways.",
              "Theconsistentperformanceacross and +0.046 over Exact-Top).",
              "This improvement\ndiverseevaluationsalsovalidatesNSA\u2019srobustness arises from two key innovations: (1) our native\nasageneral-purposearchitecture.",
              "sparseattentiondesign,whichenablesend-to-end\nLong-Context Evaluation.",
              "As shown in Fig- optimization of sparse patterns during pretrain-\nure 4, NSA achieves perfect retrieval accuracy ing, facilitates synchronized adaptation between\nacross all positions in 64k-context needle-in-a- thesparseattentionmoduleandothermodelcom-\nhaystack(Kamradt,2023)test.",
              "Thisperformance ponents; and (2) the hierarchical sparse attention\nstems from our hierarchical sparse attention de- mechanismachievesanbalancebetweenlocaland\nsign, which combines compression tokens for ef- globalinformationprocessing.",
              "ficient global context scanning, and selection to- Notably,NSAdemonstratesexceptionalperfor-\nkens for precise local information retrieval.",
              "The manceontasksrequiringcomplexreasoningover\ncoarse-grainedcompressionidentifiesrelevantcon- long contexts, achieving +0.087 and +0.051 im-\ntext blocks at low computational cost, while the provementsoverFullAttentiononmulti-hopQA\ntoken-levelattentiononselectedtokenensuresthe tasks(HPQand2Wiki),exceedingtheperformance\npreservation of critical fine-grained information.",
              "ofbaselinesoncodeunderstanding(LCC:+0.069),\nThisdesignenablesNSAtomaintainbothglobal and outperforming other methods on passage re-\nawarenessandlocalprecision.",
              "trieval(PassR-en: +0.075).",
              "Theseresultsvalidate\nWealsoevaluateNSAonLongBench(Baietal., NSA\u2019s capability to handle diverse long-context\n2023)againststate-of-the-artsparseattentionmeth- challenges, with its natively pretrained sparse at-\n23084"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b8",
            "type": "paragraph",
            "page": 8,
            "text": "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128. ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.\n9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend. As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel. Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases. context-length. Notably, the speed advantage be-\ncomes more pronounced with longer sequences.\nThisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.\nattentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation. To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.\npacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training. Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.\nfine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces. Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant). We (cid:4) (cid:5)\nsequencelength. AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark. Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length. This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore. To\nlongersequences.\nvalidatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy. Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.\ning. By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054). Theseresults\nmance. Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance. NSA\ncriticalforcomplexmathematicalderivations.\nadvances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system. tational latency. We present NSA, a hardware-\n23085",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "1000\n800\n600\n400\n200\n0\n8k 16k 32k 64k\n)sm(emiT\nForwardTimeComparison BackwardTimeComparison For both methods, we configure the model with\nGQA group g = 4, head per group h = 16 ,\nFlashAttention-2 2500\nSparseAttentionKernel query/keydimensiond = 192,andvaluedimen-\nk\nCompressionand\nWindowAttention 2000 siond = 128.",
              "ThehyperparametersforNSAare\n9.0 \u00d7 SpeedupRatio v\nkeptconsistentwithSection3.",
              "9.0 \u00d7 1500 6.0 \u00d7\n4.1 TrainingSpeed\n1000\nWecomparetheTriton-basedimplementationsof\n500\n6.3 \u00d7 3.4 \u00d7 ourNSAattentionandFullAttentionwithTriton-\n2.1\u00d7 3.8\u00d7 1.1\u00d7 2.0\u00d7\n0 based FlashAttention-2 to ensure fair speed com-\n8k 16k 32k 64k\nparison across the same backend.",
              "As shown in\nFigure5: ComparisonofTriton-basedNSAkernelwith\nFigure 5, our NSA achieve progressively greater\nTriton-basedFlashAttention-2kernel.",
              "Ourimplemen-\nspeedups as context length increases, up to 9.0\ntation significantly reduces latency across all context\n\u00d7\nlengths, with the improvement becoming more pro- forward and 6.0 backward speedup at 64k\n\u00d7\nnouncedasinputlengthincreases.",
              "context-length.",
              "Notably, the speed advantage be-\ncomes more pronounced with longer sequences.",
              "Thisspeedupstemsfromourhardware-alignedal-\ntention providing additional benefits in learning\ngorithmdesigntomaximizetheefficiencyofsparse\ntask-optimalpatterns.",
              "attentionarchitecture: (1)TheBlockwisememory\nChain-of-ThoughtReasoningEvaluation.",
              "To\naccesspatternmaximizesTensorCoreutilization,\nevaluateNSA\u2019scompatibilitywithadvanceddown-\n(2)Thedelicateloopschedulinginthekernelelim-\nstream training paradigms, we investigate its ca-\ninatesredundantKVtransfers.",
              "pacity to acquire chain-of-thought mathematical\nreasoning abilities via post-training.",
              "Given the 4.2 DecodingSpeed\nlimitedeffectivenessofreinforcementlearningon\nThe decoding speed of Attention is primarily de-\nsmaller-scalemodels,weemployknowledgedistil-\nterminedbythememoryaccessbottleneck,which\nlationfromDeepSeek-R1,conductingsupervised\niscloselytiedtotheamountofKVcacheloading.",
              "fine-tuning (SFT) with 10B tokens of 32k-length\nIneachdecodingstep,OurNSAjustneedstoload\nmathematicalreasoningtraces.",
              "Thisproducestwo\natmost s l compressiontokens,nl selectedto-\ncomparablemodels: FullAttention-R(FullAtten-\n\u2212d \u2032\nkens,andwneighbortokens,wheresisthecached\ntionbaseline)andNSA-R(oursparsevariant).",
              "We (cid:4) (cid:5)\nsequencelength.",
              "AsshowninTable4,ourmethod\nassess both models on the challenging American\nexhibits a significant reduction in latency as the\nInvitationalMathematicsExamination(AIME24)\ndecodinglengthincreases,achievingupto11.6\nbenchmark.",
              "Weuseasamplingtemperatureof0.7 \u00d7\nspeedup at 64k context-length.",
              "This advantage\nandatop-pvalueof0.95togenerate16responses\nin memory access efficiency also amplifies with\nforeachquestionandobtaintheaveragescore.",
              "To\nlongersequences.",
              "validatetheimpactofreasoningdepth,weconduct\nexperimentswithtwogenerationcontextlimits: 8k 5 Conclusion\nand 16k, measuring whether extended reasoning\nWe present NSA, a hardware-aware sparse atten-\nchainsimproveaccuracy.",
              "Examplecomparisonsof\ntionarchitectureforefficientlong-contextmodel-\nmodelpredictionsareprovidedinAppendixG.",
              "ing.",
              "By integrating hierarchical token compres-\nAs shown in Table 3, NSA-R achieves signifi-\nsionwithblockselectionwithinatrainablearchi-\ncantlyhigheraccuracythanFullAttention-Runder\ntecture, we achieve faster training and inference\nthe8kcontextsetting(+0.075),withthisadvantage\nwhilematchingorsurpassingFullAttentionperfor-\npersistingat16kcontexts(+0.054).",
              "Theseresults\nmance.",
              "Ourapproachdemonstratesthathardware-\nvalidate the pretrained sparse attention enable ef-\nalignedsparseattentioncanrivalFullAttentionim-\nficientcaptureoflong-rangelogicaldependencies\nplementationsinefficiencyandperformance.",
              "NSA\ncriticalforcomplexmathematicalderivations.",
              "advances the state-of-the-art through: superior\nbasemodelperformance,exceptionallong-context\n4 EfficiencyAnalysis\nperformance, and improved reasoning ability, all\nWeevaluatethecomputationalefficiencyofNSA while achieving significant reductions in compu-\nagainstFullAttentiononan8-GPUA100system.",
              "tational latency.",
              "We present NSA, a hardware-\n23085"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b9",
            "type": "paragraph",
            "page": 9,
            "text": "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling. By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang. 2024a. Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator. arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.\nmaintainingFullAttentionperformance. NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al. 2021. Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code. arXiv preprint\narXiv:2107.03374.\nity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.\nZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.\n2024b. Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation. arXivpreprintarXiv:2410.16179.\ntionmapsofasinglelayer. Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch. Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al. 2021. Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nforfurtherhardwareoptimization.\nDamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al. 2024. Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels. arXivpreprint\narXiv:2401.06066.\nsupported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024. Deepseek-v2: Astrong,economi-\nGrantNo. 2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.\nural Science Foundation of China (NSFC Grant\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nNumber62276002).\nsoningcapabilityinllmsviareinforcementlearning.\nPreprint,arXiv:2501.12948.\nReferences Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024. Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai. fasterinference. arXivpreprintarXiv:2412.14468.\n2023. Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints. arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.\nStanovsky,SameerSingh,andMattGardner.2019.\nDrop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs. arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.\nJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.\nProgramsynthesiswithlargelanguagemodels. arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732. Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al. 2024a.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression. arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023. Longbench: arXiv:2406.14909.\nA bilingual, multitask benchmark for long context\nunderstanding. arXivpreprintarXiv:2308.14508. YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao. 2024b. Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020. ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer. arXiv integrated retrieval and reasoning. arXiv preprint\npreprintarXiv:2004.05150. arXiv:2410.19258.\n23086",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "aligned sparse attention architecture for efficient Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xi-\nlong-context modeling.",
              "By integrating hierarchi- aozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li,\nWeiyang Liu, and Chao Huang.",
              "2024a.",
              "Sepllm:\ncaltokencompressionwithblockwisetokenselec-\nAccelerate large language models by compressing\ntionwithinatrainablearchitecture,ourarchitecture\none segment into one separator.",
              "arXiv preprint\nachievesacceleratedtrainingandinferencewhile arXiv:2412.12094.",
              "maintainingFullAttentionperformance.",
              "NSAad-\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nvances the state-of-the-art by demonstrating gen-\nYuan,HenriquePondeDeOliveiraPinto,JaredKa-\neralbenchmarkperformancematchesfull-attention\nplan, HarriEdwards, YuriBurda, NicholasJoseph,\nbaselines,exceedingmodelingcapabilityinlong- Greg Brockman, et al.",
              "2021.",
              "Evaluating large\ncontextevaluations,andenhancedreasoningabil- language models trained on code.",
              "arXiv preprint\narXiv:2107.03374.",
              "ity, allaccompaniedbymeasurablereductionsin\ncomputationallatency.",
              "ZhuomingChen,RanajoySadhukhan,ZihaoYe,Yang\nZhou,JianyuZhang,NiklasNolte,YuandongTian,\nLimitations Matthijs Douze, Leon Bottou, Zhihao Jia, et al.",
              "2024b.",
              "Magicpig: Lsh sampling for efficient llm\nIn this work, we focus on sparsity within atten- generation.",
              "arXivpreprintarXiv:2410.16179.",
              "tionmapsofasinglelayer.",
              "Exploringcross-layer\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nsparsity remains an intriguing avenue for future\nMarkChen,HeewooJun,LukaszKaiser,Matthias\nresearch.",
              "Also,ourTriton-basedimplementation,\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nwhileintroducingsomeabstractionoverheadcom- Nakano, et al.",
              "2021.",
              "Training verifiers to solve\nparedtonativeCUDAkernels,offersopportunities math word problems, 2021.",
              "URL https://arxiv.",
              "org/abs/2110.14168.",
              "forfurtherhardwareoptimization.",
              "DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,\nAcknowledgments Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y Wu, et al.",
              "2024.",
              "Deepseek-\nThe authors from Peking University (Jingyang moe: Towards ultimate expert specialization in\nYuan,JunyuLuo,andMingZhang)werepartially mixture-of-expertslanguagemodels.",
              "arXivpreprint\narXiv:2401.06066.",
              "supported by grants from the National Key Re-\nsearch and Development Program of China with\nDeepSeek-AI.2024.",
              "Deepseek-v2: Astrong,economi-\nGrantNo.",
              "2023YFC3341203andtheNationalNat- cal,andefficientmixture-of-expertslanguagemodel.",
              "ural Science Foundation of China (NSFC Grant\nDeepSeek-AI.",
              "2025.",
              "Deepseek-r1: Incentivizing rea-\nNumber62276002).",
              "soningcapabilityinllmsviareinforcementlearning.",
              "Preprint,arXiv:2501.12948.",
              "References Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana\nKlimovic,MateiZaharia,JosephEGonzalez,andIon\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury\nStoica.2024.",
              "Hashattention: Semanticsparsityfor\nZemlyanskiy,FedericoLebr\u00f3n,andSumitSanghai.",
              "fasterinference.",
              "arXivpreprintarXiv:2412.14468.",
              "2023.",
              "Gqa: Traininggeneralizedmulti-querytrans-\nformermodelsfrommulti-headcheckpoints.",
              "arXiv\nDheeruDua,YizhongWang,PradeepDasigi,Gabriel\npreprintarXiv:2305.13245.",
              "Stanovsky,SameerSingh,andMattGardner.2019.",
              "Drop: A reading comprehension benchmark re-\nJacobAustin,AugustusOdena,MaxwellNye,Maarten quiring discrete reasoning over paragraphs.",
              "arXiv\nBosma, Henryk Michalewski, David Dohan, Ellen preprintarXiv:1903.00161.",
              "Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.",
              "Programsynthesiswithlargelanguagemodels.",
              "arXiv Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan\npreprintarXiv:2108.07732.",
              "Zhang,BojuChen,TianqiWu,HongyiWang,Zix-\niao Huang, Shiyao Li, Shengen Yan, et al.",
              "2024a.",
              "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Moa: Mixture of sparse attention for automatic\nJiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao largelanguagemodelcompression.",
              "arXivpreprint\nLiu,AohanZeng,LeiHou,etal.2023.",
              "Longbench: arXiv:2406.14909.",
              "A bilingual, multitask benchmark for long context\nunderstanding.",
              "arXivpreprintarXiv:2308.14508.",
              "YuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nDong, and Wen Xiao.",
              "2024b.",
              "Not all heads mat-\nIzBeltagy,MatthewEPeters,andArmanCohan.2020.",
              "ter: Ahead-levelkvcachecompressionmethodwith\nLongformer: Thelong-documenttransformer.",
              "arXiv integrated retrieval and reasoning.",
              "arXiv preprint\npreprintarXiv:2004.05150.",
              "arXiv:2410.19258.",
              "23086"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b10",
            "type": "paragraph",
            "page": 10,
            "text": "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024. Yarn: Efficientcontextwindow\nMao Yang. 2024. Seerattention: Learning intrin- extensionoflargelanguagemodels. InICLR.Open-\nsic sparse attention in your llms. arXiv preprint Review.net.\narXiv:2410.13276.\nNoamShazeer.2019. Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed. CoRR,abs/1911.02150.\nJiaweiHan,andJianfengGao.2023. Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms. arXivpreprintarXiv:2310.01801. bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022. Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them. arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.\net al. 2024. Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext. JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530. BarisKasikci,andSongHan.2024. Quest: Query-\nawaresparsityforefficientlong-contextllminference.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.\nMantasMazeika,DawnSong,andJacobSteinhardt.\n2020. Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\nstanding. arXivpreprintarXiv:2009.03300. 2019. Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations. InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.\netal.2024. Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490. Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed. AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023. Llmlingua: Compressing ingSystems.\npromptsforacceleratedinferenceoflargelanguage\nmodels. arXivpreprintarXiv:2310.05736. YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG. Kamradt. 2023. LLMTest NeedleInAHaystack. Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.\nGitHubrepository. Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere]. languageunderstandingbenchmark. arXivpreprint\narXiv:2406.01574.\nHaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin. 2023. Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese. arXivpreprint etal.2022. Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212. soninginlargelanguagemodels. Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen. 2024. Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024. To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion. arXivpreprintarXiv:2404.14469. extrapolation for llms via dynamic token-level kv\ncacheselection. arXivpreprintarXiv:2411.02886.\nGuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024. Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion. arXivpreprintarXiv:2412.03213. andMaosongSun.2024a. Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie Jun Cai, textmemory. InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS. enceonNeuralInformationProcessingSystems.\nBernstein.2023. Generativeagents:Interactivesimu-\nlacraofhumanbehavior. InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b. Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22. inferencewithretrievalandstreamingheads. arXiv\nACM. preprintarXiv:2410.10819.\n23087",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, BowenPeng,JeffreyQuesnelle,HongluFan,andEn-\nHayden Kwok-Hay So, Ting Cao, Fan Yang, and ricoShippole.2024.",
              "Yarn: Efficientcontextwindow\nMao Yang.",
              "2024.",
              "Seerattention: Learning intrin- extensionoflargelanguagemodels.",
              "InICLR.Open-\nsic sparse attention in your llms.",
              "arXiv preprint Review.net.",
              "arXiv:2410.13276.",
              "NoamShazeer.2019.",
              "Fasttransformerdecoding: One\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, write-headisallyouneed.",
              "CoRR,abs/1911.02150.",
              "JiaweiHan,andJianfengGao.2023.",
              "Modeltellsyou\nwhattodiscard: Adaptivekvcachecompressionfor Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nllms.",
              "arXivpreprintarXiv:2310.01801.",
              "bastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakankshaChowdhery,QuocVLe,EdHChi,Denny\nGemini Team Google, Petko Georgiev, Ving Ian Lei, Zhou,etal.2022.",
              "Challengingbig-benchtasksand\nRyan Burnell, Libin Bai, Anmol Gulati, Garrett whether chain-of-thought can solve them.",
              "arXiv\nTanzer,DamienVincent,ZhufengPan,ShiboWang, preprintarXiv:2210.09261.",
              "et al.",
              "2024.",
              "Gemini 1.5: Unlocking multimodal\nunderstandingacrossmillionsoftokensofcontext.",
              "JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\narXivpreprintarXiv:2403.05530.",
              "BarisKasikci,andSongHan.2024.",
              "Quest: Query-\nawaresparsityforefficientlong-contextllminference.",
              "DanHendrycks,CollinBurns,StevenBasart,AndyZou, arXivpreprintarXiv:2406.10774.",
              "MantasMazeika,DawnSong,andJacobSteinhardt.",
              "2020.",
              "Measuringmassivemultitasklanguageunder- Philippe Tillet, Hsiang-Tsung Kung, and David Cox.",
              "standing.",
              "arXivpreprintarXiv:2009.03300.",
              "2019.",
              "Triton: an intermediate language and com-\npilerfortiledneuralnetworkcomputations.",
              "InPro-\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, ceedings of the 3rd ACM SIGPLAN International\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua WorkshoponMachineLearningandProgramming\nHan, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, Languages,pages10\u201319.",
              "etal.2024.",
              "Minference1.0: Acceleratingpre-filling\nforlong-contextllmsviadynamicsparseattention.",
              "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\narXivpreprintarXiv:2407.02490.",
              "Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser,andIlliaPolosukhin.2017.",
              "Attentionisall\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing youneed.",
              "AdvancesinNeuralInformationProcess-\nYang,andLiliQiu.2023.",
              "Llmlingua: Compressing ingSystems.",
              "promptsforacceleratedinferenceoflargelanguage\nmodels.",
              "arXivpreprintarXiv:2310.05736.",
              "YuboWang,XueguangMa,GeZhang,YuanshengNi,\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\nG.",
              "Kamradt.",
              "2023.",
              "LLMTest NeedleInAHaystack.",
              "Aaran Arulraj, Xuan He, Ziyan Jiang, et al.",
              "2024.",
              "GitHubrepository.",
              "Accessed: [InsertAccessDate Mmlu-pro:Amorerobustandchallengingmulti-task\nHere].",
              "languageunderstandingbenchmark.",
              "arXivpreprint\narXiv:2406.01574.",
              "HaonanLi,YixuanZhang,FajriKoto,YifeiYang,Hai\nZhao, YeyunGong, NanDuan, andTimothyBald- JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nwin.",
              "2023.",
              "Cmmlu: Measuring massive multitask Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,\nlanguageunderstandinginchinese.",
              "arXivpreprint etal.2022.",
              "Chain-of-thoughtpromptingelicitsrea-\narXiv:2306.09212.",
              "soninginlargelanguagemodels.",
              "Advancesinneural\ninformationprocessingsystems,35:24824\u201324837.",
              "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nPatrick Lewis, and Deming Chen.",
              "2024.",
              "Snapkv: Bai,KunFu,ZhengWang,andHuiXiong.2024.",
              "To-\nLlmknowswhatyouarelookingforbeforegenera- kenselect:Efficientlong-contextinferenceandlength\ntion.",
              "arXivpreprintarXiv:2404.14469.",
              "extrapolation for llms via dynamic token-level kv\ncacheselection.",
              "arXivpreprintarXiv:2411.02886.",
              "GuangdaLiu,ChengweiLi,JieruZhao,ChenqiZhang,\nandMinyiGuo.2024.",
              "Clusterkv: Manipulatingllm Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan\nkvcacheinsemanticspaceforrecallablecompres- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\nsion.",
              "arXivpreprintarXiv:2412.03213.",
              "andMaosongSun.2024a.",
              "Infllm:Training-freelong-\ncontextextrapolationforllmswithanefficientcon-\nJoon Sung Park, Joseph C.",
              "O\u2019Brien, Carrie Jun Cai, textmemory.",
              "InTheThirty-eighthAnnualConfer-\nMeredithRingelMorris,PercyLiang,andMichaelS.",
              "enceonNeuralInformationProcessingSystems.",
              "Bernstein.2023.",
              "Generativeagents:Interactivesimu-\nlacraofhumanbehavior.",
              "InProceedingsofthe36th GuangxuanXiao,JiamingTang,JingweiZuo,Junxian\nAnnualACMSymposiumonUserInterfaceSoftware Guo,ShangYang,HaotianTang,YaoFu,andSong\nandTechnology,UIST2023,SanFrancisco,CA,USA, Han.2024b.",
              "Duoattention:Efficientlong-contextllm\n29October2023\u20131November2023,pages2:1\u20132:22.",
              "inferencewithretrievalandstreamingheads.",
              "arXiv\nACM.",
              "preprintarXiv:2410.10819.",
              "23087"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b11",
            "type": "paragraph",
            "page": 11,
            "text": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis. 2023. Efficient streaming ing,and(3)query-awareselection. Weintroduce\nlanguagemodelswithattentionsinks. arXivpreprint\nseveralrepresentativeworksfromeachcategory.\narXiv:2309.17453.\nA.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al. 2020. Big bird: Transformers for\nafixedwindow. StreamingLLM(Xiaoetal.,2023)\nlongersequences. Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297. combines attention sinks with local windows to\nprocess continuous text streams. MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\n2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022. Star: Bootstrappingreasoningwith\nreasoning. InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling. Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences. Compared\n2022.\nwiththem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.\nWeizhuChen.2023a. Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration. InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484. Association for Computational\ndynamic KV-cache pruning. H2O (Zhang et al.,\nLinguistics.\n2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding. These methods dynamically evict\ncoding challenges. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions. FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.\ndifferent strategies to individual attention heads.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.\ntively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage. Unlike these\ninference of large language models. Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710. ratessparsitynativelyduringthetrainingphase.\nJunqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024. Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference. arXivpreprintarXiv:2410.23079. lection methods to reduce attention computation\nwhile preserving attention quality. Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024. Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels. arXivpreprintarXiv:2410.09342.\nmaxofthekeychunks. InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks. This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.\ntention. Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song (1) fixed sparse pattern, (2) dynamic token prun-\nHan, and Mike Lewis.",
              "2023.",
              "Efficient streaming ing,and(3)query-awareselection.",
              "Weintroduce\nlanguagemodelswithattentionsinks.",
              "arXivpreprint\nseveralrepresentativeworksfromeachcategory.",
              "arXiv:2309.17453.",
              "A.1 FixedSparsePattern\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey,JoshuaAinslie,ChrisAlberti,SantiagoOn- SlidingWindowisacommonlyusedapproachthat\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nallowsthequerytocomputeattentiononlywithin\nLi Yang, et al.",
              "2020.",
              "Big bird: Transformers for\nafixedwindow.",
              "StreamingLLM(Xiaoetal.,2023)\nlongersequences.",
              "Advancesinneuralinformation\nprocessingsystems,33:17283\u201317297.",
              "combines attention sinks with local windows to\nprocess continuous text streams.",
              "MoA (Fu et al.,\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.",
              "2024a)andDuoAttention(Xiaoetal.,2024b)also\nGoodman.2022.",
              "Star: Bootstrappingreasoningwith\nreasoning.",
              "InAdvancesinNeuralInformationPro- adopt similar local and sink information for long\ncessingSystems35: AnnualConferenceonNeural sequence modeling.",
              "Longformer (Beltagy et al.,\nInformationProcessingSystems2022,NeurIPS2022, 2020) interleaves local windowed attention with\nNewOrleans,LA,USA,November28\u2013December9,\nglobaltokenstoprocesslongsequences.",
              "Compared\n2022.",
              "withthem,ourNSAdoesnotrelyonpre-defined\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin sparse patterns, but learns the patterns automati-\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and\ncally,unlockingthepotentialtoutilizefullcontext.",
              "WeizhuChen.2023a.",
              "Repocoder: Repository-level\ncodecompletionthroughiterativeretrievalandgen-\nA.2 DynamicTokenPruning\neration.",
              "InProceedingsofthe2023Conferenceon\nEmpirical Methods in Natural Language Process- Some methods are designed to reduce memory\ning,EMNLP2023,Singapore,December6\u201310,2023,\nand computation costs during inference through\npages 2471\u20132484.",
              "Association for Computational\ndynamic KV-cache pruning.",
              "H2O (Zhang et al.,\nLinguistics.",
              "2023b), BUZZ (Zhao et al., 2024), and Se-\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.",
              "pLLM (Chen et al., 2024a) implement adaptive\nCodeagent: Enhancing code generation with tool-\napproachestoreduceKV-cachememoryusagedur-\nintegrated agent systems for real-world repo-level\ning decoding.",
              "These methods dynamically evict\ncoding challenges.",
              "In Proceedings of the 62nd\nAnnual Meeting of the Association for Computa- tokens deemed less important for future predic-\ntional Linguistics (Volume 1: Long Papers), ACL tions.",
              "FastGen(Geetal.,2023)andHeadKV(Fu\n2024,Bangkok,Thailand,August11\u201316,2024,pages\netal.,2024b)optimizecomputationbyassigning\n13643\u201313658.",
              "different strategies to individual attention heads.",
              "Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong SnapKV(Lietal.,2024)introducesatokenprun-\nChen,LianminZheng,RuisiCai,ZhaoSong,Yuan- ing strategy that reduces the KV-cache by selec-\ndongTian,ChristopherR\u00e9,ClarkBarrett,etal.2023b.",
              "tively retaining only the most crucial features,\nH2o: Heavy-hitter oracle for efficient generative\nenabling efficient memory usage.",
              "Unlike these\ninference of large language models.",
              "Advances in\nNeuralInformationProcessingSystems,36:34661\u2013 inference-focused approaches, our NSA incorpo-\n34710.",
              "ratessparsitynativelyduringthetrainingphase.",
              "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, and\nA.3 Query-AwareSelection\nShichaoHe.2024.",
              "Buzz: Beehive-structuredsparse\nkvcachewithsegmentedheavyhittersforefficient Other works focus on query-dependent token se-\nllminference.",
              "arXivpreprintarXiv:2410.23079.",
              "lection methods to reduce attention computation\nwhile preserving attention quality.",
              "Quest (Tang\nZihan Zhou, Chong Li, Xinyi Chen, Shuo Wang,\net al., 2024) employs a blockwise selection strat-\nYu Chao, Zhili Li, Haoyu Wang, Rongqiao An,\nQiShi,ZhixingTan,etal.2024.",
              "Llm mapreduce: egywhereeachchunk\u2019simportanceisestimatedby\n\u00d7\nSimplifiedlong-sequenceprocessingusinglargelan- product between query and coordinate-wise min-\nguagemodels.",
              "arXivpreprintarXiv:2410.09342.",
              "maxofthekeychunks.",
              "InfLLM(Xiaoetal.,2024a)\ncombinesfixedpatternswithretrievalbymaintain-\nA RelatedWorks\ning attention sinks, local context, and retrievable\nWereviewexistingapproachesthatimprovetheef- chunks.",
              "This method selects representative keys\nficiencyofattentioncomputationthroughsparseat- from each chunk to estimate chunk importance.",
              "tention.",
              "Thesemethodscanbebroadlycategorized HashAttention(Desaietal.,2024)formulatespiv-\ninto three groups based on their core strategies: otaltokenidentificationasarecommendationprob-\n23088"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b12",
            "type": "paragraph",
            "page": 12,
            "text": "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions. ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity. MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads. For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring. eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset. Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation. Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding. fromallqueryheadswithinthesameGQAgroup.\nThisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh. Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels. How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.\nFull Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages. Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends. Thismotivates\nlenses.\nus to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.\ntion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity. Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g. attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling. In tion trajectory. As demonstrated by Chen et al.\ncontrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity. These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference. (2)TrainingEfficiency\nputationalcostscomparabletoFullAttention. The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.\nthesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning. reinforcementlearning. However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture. Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "lem by mapping queries and keys to Hamming adapt to modern decoding efficient architectures\nspace using learned functions.",
              "ClusterKV (Liu like Mulitiple-Query Attention (MQA) (Shazeer,\netal.,2024)achievessparsitybyfirstlyclustering 2019) and Grouped-Query Attention (GQA)\nkeysandthenselectingthemostrelevantclusters (Ainslieetal.,2023),whichsignificantlyreduced\nfor attention computation based on query-cluster thememoryaccessbottleneckduringdecodingby\nsimilarity.",
              "MInference(Jiangetal.,2024)andTo- sharing KV across multiple query heads.",
              "For in-\nkenSelect(Wuetal.,2024)selectKVpairsforcom- stance,inapproacheslikeQuest(Tangetal.,2024),\nputationbasedontoken-levelimportancescoring.",
              "eachattentionheadindependentlyselectsitsKV-\nSeerAttention(Gaoetal.,2024)separatesqueries cachesubset.",
              "Althoughitdemonstratesconsistent\nand keys into spatial blocks and performs block- computationsparsityandmemoryaccesssparsity\nwiseselectionforefficientcomputation.",
              "Compared inMulti-HeadAttention(MHA)models,itpresents\nto these methods, our NSA achieves hardware- a different scenario in models based-on architec-\naligned sparse attention computation throughout tureslikeGQA,wherethememoryaccessvolume\nthefullmodellifecycle,includingtraining,prefill- ofKV-cachecorrespondstotheunionofselections\ning,anddecoding.",
              "fromallqueryheadswithinthesameGQAgroup.",
              "Thisarchitecturalcharacteristicmeansthatwhile\nB RethinkingSparseAttentionMethods thesemethodscanreducecomputationoperations,\ntherequiredKV-cachememoryaccessremainsrel-\nModern sparse attention methods have made sig-\nativelyhigh.",
              "Thislimitationforcesacriticalchoice:\nnificantstridesinreducingthetheoreticalcompu-\nwhilesomesparseattentionmethodsreducescom-\ntationalcomplexityoftransformermodels.",
              "How-\nputation,itsscatteredmemoryaccesspatterncon-\never, most approaches predominantly apply spar-\nflicts with efficient memory access design from\nsityduringinferencewhileretainingapretrained\nadvancedarchitectures.",
              "Full Attention backbone, potentially introducing\nThese limitations arise because many existing\narchitectural bias that limits their ability to fully\nsparse attention methods focus on KV-cache re-\nexploitsparseattention\u2019sadvantages.",
              "Beforeintro-\nductionortheoreticalcomputationreduction,but\nducingournativesparsearchitecture,wesystemat-\nstruggletoachievesignificantlatencyreductionin\nicallyanalyzetheselimitationsthroughtwocritical\nadvancedframeworksorbackends.",
              "Thismotivates\nlenses.",
              "us to develop algorithms that combine both ad-\nvancedarchitecturalandhardware-efficientimple-\nB.1 TheIllusionofEfficientInference\nmentationtofullyleveragesparsityforimproving\nDespite achieving sparsity in attention computa- modelefficiency.",
              "tion,manymethodsfailtoachievecorresponding\nreductions in inference latency, primarily due to B.2 TheMythofTrainableSparsity\ntwochallenges: Our pursuit of native trainable sparse attention\nPhase-Restricted Sparsity.",
              "Methods such as is motivated by two key insights from analyz-\nH2O(Zhangetal.,2023b)applysparsityduringau- ing inference-only approaches: (1) Performance\ntoregressivedecodingwhilerequiringcomputation- Degradation: Applying sparsity post-hoc forces\nally intensive pre-processing (e.g.",
              "attention map models to deviate from their prtrained optimiza-\ncalculation, index building) during prefilling.",
              "In tion trajectory.",
              "As demonstrated by Chen et al.",
              "contrast,approacheslikeMInference(Jiangetal., (2024b),top20%attentioncanonlycover70%of\n2024) focus solely on prefilling sparsity.",
              "These thetotalattentionscores,renderingstructureslike\nmethodsfailtoachieveaccelerationacrossallin- retrieval heads in prtrained models vulnerable to\nferencestages,asatleastonephaseremainscom- pruningduringinference.",
              "(2)TrainingEfficiency\nputationalcostscomparabletoFullAttention.",
              "The Demands: Efficient handling of long-sequence\nphasespecializationreducesthespeedupabilityof trainingiscrucialformodernLLMdevelopment.",
              "thesemethodsinprefilling-dominatedworkloads Thisincludesbothpretrainingonlongerdocuments\nlikebooksummarizationandcodecompletion,or toenhancemodelcapacity, andsubsequentadap-\ndecoding-dominatedworkloadslikelongchain-of- tationphasessuchaslong-contextfine-tuningand\nthought(Weietal.,2022)reasoning.",
              "reinforcementlearning.",
              "However,existingsparse\nIncompatibilitywithAdvancedAttentionAr- attentionmethodsprimarilytargetinference,leav-\nchitecture.",
              "Somesparseattentionmethodsfailto ingthecomputationalchallengesintraininglargely\n23089"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b13",
            "type": "paragraph",
            "page": 13,
            "text": "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel. Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.\nunaddressed. Thislimitationhindersthedevelop- sparseattentionmechanisms. Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training. Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.\nchallenges:\nNon-Trainable Components. Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph. and MoE. The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560. For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns. tentionheads. Foreachhead,thequery, key, and\nInefficientBack-propagation. Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively. For MoE, we utilize\nv\npracticaltraininginefficiencies. Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6. To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation. Thisnon- isreplacedbyanMLPintheformofSwiGLU.\ncontiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.\nAsaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency. stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel. Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.\n23090",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n0 10000 20000 30000 40000 50000 60000\nstep\nssol\nTainingLossCurve\nFullAttention\nNSA\nFigure6: PretraininglosscomparisonbetweenFullAttentionandourNSAon27B-parametermodel.",
              "Bothmodels\nexhibitstableconvergence,withNSAachievingmarginallylowerlossvalues.",
              "unaddressed.",
              "Thislimitationhindersthedevelop- sparseattentionmechanisms.",
              "Inthemainpaper,we\nmentofmorecapablelong-contextmodelsthrough proposedNSA,anativelysparseattentionframe-\nefficient training.",
              "Additionally, efforts to adapt workthataddressesbothcomputationalefficiency\nexistingsparseattentionsfortrainingalsoexpose andtrainingrequirements.",
              "challenges:\nNon-Trainable Components.",
              "Discrete opera-\nC DetailsofBackbone\ntionsinmethodslikeClusterKV(Liuetal.,2024)\n(includesk-meansclustering)andMagicPIG(Chen\netal.,2024b)(includesSimHash-basedselecting) WeadoptatransformerframeworkbasedonGQA\ncreatediscontinuitiesinthecomputationalgraph.",
              "and MoE.",
              "The model consists of 30 layers with\nThesenon-trainablecomponentspreventgradient a hidden dimension of 2560.",
              "For GQA, we set\nflowthroughthetokenselectionprocess,limiting the number of groups to 4, with a total of 64 at-\nthemodel\u2019sabilitytolearnoptimalsparsepatterns.",
              "tentionheads.",
              "Foreachhead,thequery, key, and\nInefficientBack-propagation.",
              "Sometheoreti- valuedimensionsareconfiguredasd q = d k = 192\ncallytrainablesparseattentionmethodssufferfrom and d = 128, respectively.",
              "For MoE, we utilize\nv\npracticaltraininginefficiencies.",
              "Token-granularse- the DeepSeekMoE (Dai et al., 2024; DeepSeek-\nlection strategy used in approaches like HashAt- AI,2024)structure,with72routedexpertsand2\ntention (Desai et al., 2024) leads to the need to shared experts, and set the top-k experts to 6.",
              "To\nloadalargenumberofindividualtokensfromthe ensuretrainingstability,theMoEinthefirstlayer\nKVcacheduringattentioncomputation.",
              "Thisnon- isreplacedbyanMLPintheformofSwiGLU.",
              "contiguousmemoryaccesspreventsefficientadap-\ntationoffastattentiontechniqueslikeFlashAtten-\nD Traininglosscurve\ntion,whichrelyoncontiguousmemoryaccessand\nblockwisecomputationtoachievehighthroughput.",
              "Asaresult,implementationsareforcedtofallback Asshownin Figure6,thepretraininglosscurveof\ntolowhardwareutilization,significantlydegrading ourNSAandFullAttentionbaselinedemonstrates\ntrainingefficiency.",
              "stableandsmoothdecline,withNSAconsistently\noutperformingtheFullAttentionmodel.",
              "Thisindi-\nB.3 NativeSparsityasanImperative\ncates that our sparse model effectively maintains\nTheselimitationsininferenceefficiencyandtrain- trainingstabilitywhileachievingsuperiorperfor-\ningviabilitymotivateourfundamentalredesignof mance.",
              "23090"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b14",
            "type": "paragraph",
            "page": 14,
            "text": "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.\nHowever,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies. We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024). ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions. Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols. Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.\nOtherBlockwiseSelectionStrategies. Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a). These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq . However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance. Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention. For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores. Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction. Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand. OurNSAachievesbetterperformance.\nVisualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer. Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.\nefficientdecoding. Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024). For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters. We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection. Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.\nF Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8. The\n23091",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "E ChallengeswithAlternativeToken\n3.2\nSelectionStrategies\nBeforedesigningNSA,weexploredadaptingexist- 3.0\ningsparseattentionmethodstothetrainingstage.",
              "However,theseattemptsencounteredvariouschal- 2.8\nlenges, prompting us to design a different sparse\nattentionarchitecture: 2.6\nKey-Clustering Based Strategies.",
              "We ex-\n2.4\namined clustering-based strategies like Clus-\nterKV(Liuetal.,2024).",
              "ThesemethodsstoreKeys\n2.2\nand Values from the same cluster in contiguous 0 5000 10000 15000 20000 25000 30000\nstep\nmemoryregions.",
              "Whiletheoreticallyfeasiblefor\ntrainingandinference,theyfacethreesignificant\nchallenges: (1)Non-trivialcomputationaloverhead\nintroducedbydynamicclusteringmechanisms;(2)\nOperatoroptimizationdifficultiesexacerbatedby\ninter-clusterimbalances,especiallyinMixture-of-\nExperts(MoE)systems,whereskewedExpertPar-\nallelism (EP) group execution times lead to per-\nsistent load imbalances; (3) Implementation con-\nstraintsarisingfromtheneedformandatoryperi-\nodicreclusteringandchunk-sequentialtrainingpro-\ntocols.",
              "Thesecombinedfactorscreatesubstantial\nbottlenecks, significantly limiting their effective-\nnessforreal-worlddeployment.",
              "OtherBlockwiseSelectionStrategies.",
              "Wealso\nconsidered blockwise key, value selection strate-\ngiesdifferentfromNSA,suchasQuest(Tangetal.,\n2024) and InfLLM (Xiao et al., 2024a).",
              "These\nmethodsrelyoncomputinganimportancescorefor\neachKVblockandselectingthetop-nblocksbased\nontheirsimilaritywithq .",
              "However,existingmeth-\nt\nodsfacetwocriticalissues: (1)Sincetheselection\noperation is non-differentiable, importance score\ncomputation based on neural networks relies on\nauxiliaryloss,whichincreasesoperatoroverhead\nandoftendegradesmodelperformance;(2)Heuris-\ntic parameter-free importance score computation\nstrategysufferfromlowrecallrates,leadingtosub-\noptimalperformance.",
              "Weevaluatebothapproaches\nona3B-parametermodelwithsimilararchitecture\nand compare their loss curve with NSA and Full\nAttention.",
              "For the auxiliary loss-based selection\nmethod,weintroduceadditionalqueriesforeach\ntokenandrepresentativekeysforeachblocktoes-\ntimatetheblockimportancescores.",
              "Wecompute\nblock-level supervision signals by mean-pooling\nattention scores within each key block, and use\nKLdivergencetosuperviseblockimportancepre-\ndiction.",
              "Wemaintainindividualquerygranularity\ninsteadofblock-averagedqueriestoaccommodate\nssol\nTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSA\nFigure7: Comparetraininglossona3B-parametermodel\nwithFullAttentionanddifferenttokenselectionstrategies\nand.",
              "OurNSAachievesbetterperformance.",
              "Visualization for Attention Score Block Pattern\nposition\nnoitisop\nFigure8: VisualizationofAttentionMaponaFull\nAttentiontransformer.",
              "Light-coloredregionsindi-\ncatehigherattentionvalues.Asshowninthefigure,\nattentionscoresexhibitblockwiseclusteringdistri-\nbution.",
              "efficientdecoding.",
              "Thisauxiliaryloss-basedimpor-\ntanceestimationsharesconceptualsimilaritywith\nSeerAttention (Gao et al., 2024).",
              "For the heuris-\nticparameter-freeselectionmethod,followingthe\nstrategy of Quest, we implement direct selection\nusingtheproductbetweenqueriesandcoordinate-\nwise min-max of the key chunks, without intro-\nducing additional parameters.",
              "We also explore a\ncold-starttrainingapproachwhereFullAttentionis\nappliedfortheinitial1000stepsbeforetransition-\ningtotheheuristicblockwiseselection.",
              "Asshown\nin Figure7,bothmethodsexhibitedinferiorloss.",
              "F Visualization\nToexplorepotentialpatternsintransformeratten-\ntion distributions and seek inspiration for our de-\nsign,wevisualizetheattentionmapfromourpre-\ntrained27BFullAttentionmodelinFigure8.",
              "The\n23091"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b15",
            "type": "paragraph",
            "page": 15,
            "text": "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores. This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach. The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.\nThisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.\n23092",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "visualizationrevealsinterestingpatternswhereat-\ntention scores tend to exhibit blockwise cluster-\ning characteristics, with nearby keys often show-\ning similar attention scores.",
              "This observation in-\nspired our design of NSA, suggesting that select-\ning key blocks based on spatial continuity might\nbe a promising approach.",
              "The blockwise cluster-\ningphenomenonindicatesthattokensadjacentin\nthesequencemaysharecertainsemanticrelation-\nships with query tokens, though the exact nature\noftheserelationshipsrequiresfurtherinvestigation.",
              "Thisobservationmotivatedustoexploreasparse\nattentionmechanismthatoperatesoncontinuous\ntokenblocksratherthanindividualtokens,aiming\ntoenhancecomputationalefficiencyandpreserve\nhigh-attentionpattern.",
              "23092"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b16",
            "type": "paragraph",
            "page": 16,
            "text": "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers. Findm+n.\n2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ... /think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1. log x = 1 2. log y = 1 3. log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1. x = \u221a22. y = 21/3 3. z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1. x = \u221a2 yz 2. y = 21/3 xz 3. z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12. -\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8. -Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.\n\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.\n33\n23093",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "G ExampleofAIMEResults\nPrompt:\n\u201c|User|Letx,y andz bepositiverealnumbersthatsatisfythefollowingsystemofequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nThenthevalueof log (x4y3z2) is m wheremandnarerelativelyprimepositiveintegers.",
              "Findm+n.",
              "2 n\nPleasereasonstepbystep,andputyourfinalanswerwithin .|Assistant|\u201d\n(cid:12) (cid:12)\n(cid:12) (cid:12)\nOurResult:\nthink ...",
              "/think (2275tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemofequations:\n1.",
              "log x = 1 2.",
              "log y = 1 3.",
              "log z = 1\n2 yz 2 2 xz 3 2 xy 4\nWecon(cid:16)vert(cid:17)theselogarithm(cid:0)ic(cid:1)equationstoth(cid:16)eir(cid:17)exponentialforms:\n1.",
              "x = \u221a22.",
              "y = 21/3 3.",
              "z = 21/4\nyz xz xy\nExpressingeachvariableintermsofx,y,andz:\n1.",
              "x = \u221a2 yz 2.",
              "y = 21/3 xz 3.",
              "z = 21/4 xy\n\u00b7 \u00b7 \u00b7\nSubstitutingtheseexpressionsintoeachother,wefind:\n-Fromx = \u221a2 yz andy = 21/3 xz,wegetx = \u221a2 (21/3 xz) z,leadingtoz = 2 5/12.",
              "-\n\u2212\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nSubstitutingzintotheequationfory,wegety = 2 3/8.",
              "-Substitutingzandyintotheequation\n\u2212\nforx,wegetx = 2 7/24.",
              "\u2212\nNext,wecomputex4y3z2:\n-x4 = (2 7/24)4 = 2 7/6 -y3 = (2 3/8)3 = 2 9/8 -z2 = (2 5/12)2 = 2 5/6\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\nMultiplyingthesetogether:\nx4y3z2 = 2 7/6 2 9/8 2 5/6 = 2 75/24 = 2 25/8\n\u2212 \u2212 \u2212 \u2212 \u2212\n\u00b7 \u00b7\nTakingthelogarithmbase2:\nlog (x4y3z2) = 25/8\n2 \u2212\nTheabsolutevalueis:\nlog (x4y3z2) = 25/8\n| 2 |\nThus,m = 25andn = 8,givingm+n = 33.",
              "33\n23093"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b17",
            "type": "paragraph",
            "page": 17,
            "text": "BaselineResult:\nthink ... /think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1. x = \u221a2 yz 2. y = 2 1 3 xz 3. z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.\nTherefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z). Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion. Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "BaselineResult:\nthink ...",
              "/think (9392tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventhesystemoflogarithmicequations:\nx 1\nlog =\n2 yz 2\n(cid:18) (cid:19)\ny 1\nlog =\n2 xz 3\n(cid:16) (cid:17)\nz 1\nlog =\n2 xy 4\n(cid:18) (cid:19)\nWeconverttheseequationstoexponentialform:\nx\n\u221a2 = = x = \u221a2 yz\nyz \u21d2 \u00b7\ny\n1 1\n23 = = y = 23 xz\nxz \u21d2 \u00b7\nz\n1 1\n24 = = z = 24 xy\nxy \u21d2 \u00b7\nSubstitutingtheseintoeachother,wefind:\n1.",
              "x = \u221a2 yz 2.",
              "y = 2 1 3 xz 3.",
              "z = 2 1 4 xy\n\u00b7 \u00b7 \u00b7\nBysolvingtheseequationsstep-by-step,wefind:\n-z = 2 5/12 -xy = 2 2/3 -y = 2 1/24 -x = 21/24\n\u2212 \u2212 \u2212\nHowever,uponverifyingthethirdequation,werealizethesystemmightnothaveasolution.",
              "Therefore,weusematrixinversiontosolvethesystem:\nlog (x) log (y) log (z) = 1\n2 \u2212 2 \u2212 2 2\nlog (y) log (x) log (z) = 1\n\uf8f1 2 \u2212 2 \u2212 2 3\n\uf8f4 \uf8f2log (z) log (x) log (y) = 1\n2 \u2212 2 \u2212 2 4\n\uf8f4\nLetA = log (x),B = log \uf8f3(y),C = log (z).",
              "Thesystembecomes:\n2 2 2\nA B C = 1\n\u2212 \u2212 2\nB A C = 1\n\uf8f1 \u2212 \u2212 3\n\uf8f4 \uf8f2C A B = 1\n\u2212 \u2212 4\n\uf8f4\nWesolvethissystemusingCramer\u2019s\uf8f3ruleandmatrixinversion.",
              "Thedeterminantofthematrixis\n-2,andthesolutionsare:\n61 13 1\nA = , B = , C =\n48 16 \u22126\nThus,wehave:\nx = 261/48, y = 213/16, z = 2 1/6\n\u2212\n23094"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b18",
            "type": "paragraph",
            "page": 18,
            "text": "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16. Therefore,m+n = 115+16 =\n16\n131.\n131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?\nPleasereasonstepbystep,andputyourfinalanswerwithin . |Assistant|\u201d\nOurResult:\nthink ... /think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1. log (yx) = 10\nx\n2. log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.\nForthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x). Then,log (y) = 1. Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Weneedtofind log (x4y3z2) :\n2\n(cid:12) (cid:12)\n(cid:12) (cid:12) 61 13 1\n4log x+3log y+2log z = 4 +3 +2\n2 2 2 \u00b7 48 \u00b7 16 \u00b7 \u22126\n(cid:18) (cid:19)\nConvertingtoacommondenominator(48):\n61 244 13 117 1 16\n4 = , 3 = , 2 =\n\u00b7 48 48 \u00b7 16 48 \u00b7 \u22126 \u221248\n(cid:18) (cid:19)\nSummingthese:\n244 117 16 345 115\n+ = =\n48 48 \u2212 48 48 16\nThefraction 115 isinsimplestform,som = 115andn = 16.",
              "Therefore,m+n = 115+16 =\n16\n131.",
              "131\nPrompt:\n\u201c|User|Realnumbersxandy withx,y > 1satisfylog (yx) = log (x4y) = 10.Whatisthe\nx y\nvalueofxy?",
              "Pleasereasonstepbystep,andputyourfinalanswerwithin .",
              "|Assistant|\u201d\nOurResult:\nthink ...",
              "/think (15147tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiventheequations:\n1.",
              "log (yx) = 10\nx\n2.",
              "log (x4y) = 10\ny\nWestartbyapplyingthelogarithmpowerruletobothequations.",
              "Forthefirstequation:\nlog (yx) = x log (y) = 10\nx \u00b7 x\nLetz = log (x).",
              "Then,log (y) = 1.",
              "Substitutingthisintotheequation,weget:\ny x z\n1\nx = 10 = x = 10z\n\u00b7 z \u21d2\nForthesecondequation:\nlog (x4y) = y log (x4) = 10\ny \u00b7 y\n23095"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b19",
            "type": "paragraph",
            "page": 19,
            "text": "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1. x = 10z\n5/2\n2. y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .\nBaselineResult:\nthink ... /think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.\nx y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.\n1. log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny . Sincelog x = 9,wehavelog y = 4. Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2. log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.\nx 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Usingthepowerrule,weget:\ny 4log (x) = 10 = 4ylog (x) = 10\n\u00b7 y \u21d2 y\nSincelog (x) = z,thissimplifiesto:\ny\n5\n4yz = 10 = yz =\n\u21d2 2\nWenowhavetwoexpressions:\n1.",
              "x = 10z\n5/2\n2.",
              "y =\nz\n5/2\nSubstitutingx = 10z andy = intotheproductxy,weget:\nz\n5/2 5 5\nxy = (10z) = 10z = 10 = 25\n\u00b7 z \u00b7 2z \u00b7 2\n(cid:18) (cid:19)\nThus,thevalueofxy is 25 .",
              "BaselineResult:\nthink ...",
              "/think (16223tokens)\n\u27e8 \u27e9 \u27e8 \u27e9\nGiven the problem where real numbers x and y with x,y > 1 satisfy the equations\nlog (yx) = 10andlog (x4y) = 10,weneedtofindthevalueofxy.",
              "x y\nFirst,weconvertthelogarithmicequationstotheirexponentialformsusingthechangeofbase\nformula.",
              "1.",
              "log (yx) = 10\nx\nxlog (y) = 10\nx\nUsingthechangeofbaseformula,log (y) = lny .",
              "Sincelog x = 9,wehavelog y = 4.",
              "Thus,\nx lnx y 4 x 9\n4 10 9 90 45\nx = 10 = x = \u00b7 = =\n\u00b7 9 \u21d2 4 4 2\n2.",
              "log (x4y) = 10\ny\n9\n4log x+1 = 10 = 4log x = 9 = log x =\ny \u21d2 y \u21d2 y 4\nAgain,usingthechangeofbaseformula,log y = 4.",
              "x 9\nNext,weexpressxandy intermsofeachother:\n4/9\n45 45\nx = and y =\n2 2\n(cid:18) (cid:19)\n23096"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          },
          {
            "id": "b20",
            "type": "paragraph",
            "page": 20,
            "text": "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097",
            "caption": null,
            "key_points": null,
            "image_path": null,
            "table_cells": null,
            "bbox": null,
            "provenance": null,
            "sentences": [
              "Wethencomputetheproductxy:\n4/9 1+4/9 13/9\n45 45 45 45\nxy = = =\n2 \u00b7 2 2 2\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\nHowever, upon re-evaluating and using another approach by setting x = 2a and y = 2b, we\nfind:\nxlog y 4 45\nlog (yx) = 10 = x = 10 = x = 10 = x =\nx \u21d2 log y \u21d2 \u00b7 9 \u21d2 2\nx\n4log x+1 9\nlog (x4y) = 10 = y = 10 = 4 +1 = 10 = 9+1 = 10 consistent\ny \u21d2 log x \u21d2 \u00b7 4 \u21d2\ny\nFinally,wefindthattheproductxy simplifiesto:\n45\nxy =\n2\n(cid:18) (cid:19)\n23097"
            ],
            "metadata": {
              "numbers": [],
              "entities": [],
              "markers": []
            }
          }
        ],
        "explanations": [
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.', rationale='The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.', evidence=[EvidenceItem(text='NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14'), EvidenceItem(text='The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.', rationale='Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.', evidence=[EvidenceItem(text='The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)",
          "ExplanationCard(kind='explanation.v1', version='1.0.0', confidence=0.5, provenance=[], claim='The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.', rationale='The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.', evidence=[EvidenceItem(text='Problem motivation and limitations of existing sparse attention', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='NSA\u2019s overall framework and three-branch sparse attention design', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Detailed algorithmic components: compression, selection, sliding window, and gating', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Experimental setup and backbone architecture', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on general benchmarks', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Results on long-context benchmarks and retrieval', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Chain-of-thought reasoning results', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Efficiency measurements: training, prefill, and decoding', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Relation to prior sparse attention methods and design choices', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Training behavior and explored but discarded selection strategies', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14'), EvidenceItem(text='Limitations and future directions', source='paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14')], summary=None)"
        ],
        "terms": [
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.95, provenance=[], term='sparse attention', definition='A way to speed up transformer models by computing attention scores for only a small, carefully chosen subset of tokens instead of every single one.', aliases=['sparsified attention', 'selective attention'], examples=['Sparse attention cuts the cost of summarising a 100-page report by skipping unrelated paragraphs.', 'Instead of looking at all 64 000 tokens, the model uses sparse attention to focus on just 2 500.'], sources=['b1', 'b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='arithmetic intensity', definition='The ratio of maths operations (like multiplications) to memory reads; high intensity means the GPU is kept busy computing rather than waiting for data.', aliases=['compute-to-memory ratio', 'FLOP/byte ratio'], examples=['Rewriting the kernel raised arithmetic intensity so the GPU finished 30 % faster.', 'When intensity is too low the task becomes memory-bound and extra cores don\u2019t help.'], sources=['b2', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.92, provenance=[], term='token compression', definition='Grouping neighbouring tokens into a single summary representation so the attention layer has fewer items to process.', aliases=['block compression', 'chunk summarisation'], examples=['Token compression turns each 32-token block into one vector, shrinking the key list by 32\u00d7.', 'After compression the model still sees the gist of the paragraph without reading every word.'], sources=['b3', 'b4'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.88, provenance=[], term='blockwise selection', definition='Choosing entire continuous chunks of tokens for attention instead of scattering individual positions, keeping memory access patterns fast on GPUs.', aliases=['chunk selection', 'block-based sparsity'], examples=['Blockwise selection picks the 16 most relevant sentence blocks instead of 500 separate words.', 'Because GPUs like contiguous data, blockwise selection gives a bigger real-world speed-up than random sparsity.'], sources=['b4', 'b5'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.93, provenance=[], term='KV-cache', definition='The stored list of Key and Value vectors from earlier tokens that the model re-uses when generating each new word, avoiding recalculation.', aliases=['key-value store', 'past-context cache'], examples=['A 64 k-token KV-cache can occupy several gigabytes, so methods try to evict unneeded entries.', 'During decoding the GPU loads only the slimmed-down KV-cache instead of recomputing everything.'], sources=['b2', 'b4', 'b8'])",
          "TermCard(kind='term.v1', version='1.0.0', confidence=0.9, provenance=[], term='natively trainable', definition='Designed so that the sparse pattern is learned jointly with the rest of the model during training, not bolted on later.', aliases=['training-aware', 'end-to-end trainable'], examples=['Because NSA is natively trainable, gradients flow through the token-selection step.', 'Unlike post-hoc pruning, a natively trainable sparse layer keeps improving as the model sees more data.'], sources=['b1', 'b2', 'b12'])"
        ],
        "relevance_notes": [
          "RelevanceNote(kind='relevance.v1', version='1.0.0', confidence=0.0, provenance=[], target='System Message', rationale='Relevance analysis unavailable: LLM returned no usable JSON content.', score=0.0)"
        ],
        "review_report": "ReviewReport(kind='review_report.v1', version='1.0.0', confidence=0.7, provenance=[], overall=0.8319400601173814, criteria=ReviewCriteria(kind='review_criteria.v1', version='1.0.0', confidence=0.7, provenance=[], accuracy=0.7, clarity=0.7277602404695259, completeness=1.0, safety=0.9), comments=['[readability] Aggregate readability score: 0.73 in [0,1].'], actions=[])",
        "public_brief_md_path": "artifacts\\reports\\latest.md",
        "planner_plan_spec.replan": {
          "strategy": "llm_planner.v1",
          "steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "enable_history": false,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested.",
          "should_replan": false,
          "replan_steps": null,
          "replan_reason": ""
        },
        "planner_report": {
          "strategy": "llm_planner.v1",
          "enable_history": false,
          "initial_steps": [
            "parse",
            "translate",
            "jargon",
            "citizen",
            "review",
            "brief"
          ],
          "replan_steps": null,
          "refine_used": false,
          "replan_reason": null,
          "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested."
        },
        "public_brief": {
          "kind": "public_brief.v1",
          "version": "1.0.0",
          "confidence": 1.0,
          "provenance": [],
          "title": "InterLines - Public Brief",
          "summary": "NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.\n\nNSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.\n\nThe paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.",
          "sections": [
            {
              "heading": "Key Insights",
              "body": "Generated from expert explanations.",
              "bullets": [
                "NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.",
                "NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.",
                "The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches."
              ]
            }
          ],
          "explanations": [
            {
              "kind": "explanation.v1",
              "version": "1.0.0",
              "confidence": 0.5,
              "provenance": [],
              "claim": "NSA is a natively trainable, hardware-aligned sparse attention mechanism that matches or surpasses full attention on benchmarks while greatly speeding up long-context training and inference.",
              "rationale": "The paper introduces NSA, which combines hierarchical sparse token modeling with Triton-based kernels to reduce computation and memory access, and shows experimentally that a 27B model trained with NSA maintains or exceeds full attention performance on general, long-context, and reasoning benchmarks while achieving up to ~9\u00d7 training forward, ~6\u00d7 backward, and ~11.6\u00d7 decoding speedups at 64k context.",
              "evidence": [
                {
                  "text": "NSA introduces a hierarchical sparse attention architecture combining token compression, token selection, and sliding window branches to preserve both global and local information with far fewer key\u2013value pairs per query.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                },
                {
                  "text": "NSA is explicitly designed to be hardware-aligned, with arithmetic-intensity-aware algorithms and custom Triton kernels that exploit grouped-query attention KV-sharing and blockwise memory access to reduce latency in prefill, training, and decoding.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                },
                {
                  "text": "NSA is natively trainable end-to-end, avoiding non-differentiable or post-hoc sparsification, and achieves stable convergence comparable to full attention on a 27B-parameter backbone.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                },
                {
                  "text": "On general benchmarks (knowledge, reasoning, coding), the NSA-pretrained model matches or slightly exceeds full attention and outperforms prior sparse methods where comparable.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                },
                {
                  "text": "On long-context evaluations (Needle-in-a-Haystack, LongBench) NSA attains perfect 64k retrieval and the best average LongBench score, improving especially on multi-hop and code-related tasks over full attention.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                },
                {
                  "text": "With SFT for chain-of-thought math reasoning, NSA-based models outperform full-attention baselines on AIME24 under both 8k and 16k generation limits, indicating that sparse pretraining does not harm\u2014and may help\u2014deep reasoning.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                },
                {
                  "text": "Measured on A100 GPUs, NSA\u2019s kernels achieve large speedups over FlashAttention-2 for long sequences (up to 9\u00d7 forward, 6\u00d7 backward), and the decoding memory-access reduction implies up to ~11.6\u00d7 decoding speedup at 64k context.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                },
                {
                  "text": "The authors argue that many existing sparse attention methods fail to translate theoretical sparsity into real-world speed or to support training; NSA is proposed specifically to close these gaps.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b13, b14"
                }
              ],
              "summary": null
            },
            {
              "kind": "explanation.v1",
              "version": "1.0.0",
              "confidence": 0.5,
              "provenance": [],
              "claim": "NSA is a hierarchical, hardware-optimized sparse attention mechanism that can be trained end-to-end and delivers strong accuracy with substantial speedups for long-context language modeling.",
              "rationale": "Across the paper, the authors motivate, define, implement, and empirically evaluate NSA, and systematically compare it to full attention and prior sparse methods, highlighting both design and performance.",
              "evidence": [
                {
                  "text": "The motivation is that long-context LLMs are increasingly important, but standard softmax attention dominates latency (70\u201380% at 64k decoding), and many existing sparse methods either only sparsify part of the pipeline, are not hardware-aligned, or are hard to train end-to-end.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "NSA addresses this by replacing the full key\u2013value history with a query-dependent, compact set of representations organized into three paths: compressed coarse-grained blocks, top-n selected fine-grained blocks, and a sliding local window, with learned gates that combine their contributions.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "To realize practical speed, NSA designs blockwise, group-centric kernels in Triton that share KV across GQA heads, load continuous KV blocks, and schedule work to maintain high arithmetic intensity and minimize redundant memory transfers, particularly during decoding.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "The method is instantiated in a 27B-parameter GQA+MoE transformer trained natively with NSA on up to 32k contexts, with stable training loss similar to or slightly better than full attention, demonstrating that the sparse mechanism is trainable from scratch.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "On general benchmarks (MMLU, GSM8K, MATH, DROP, MBPP, HumanEval), the NSA model matches or slightly improves on a full-attention baseline, with notable gains on reasoning tasks, suggesting that the sparsity induces more focused attention without harming capability.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "On long-context tasks, NSA achieves perfect 64k Needle-in-a-Haystack retrieval and the best average LongBench performance among compared methods, outperforming full attention particularly on multi-hop QA and code-understanding subsets, which rely on accurate selection of relevant context.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Further, when both NSA and full-attention backbones are SFT\u2019d with chain-of-thought supervision (distilled from DeepSeek-R1) for AIME24, the NSA variant yields higher scores at both 8k and 16k generation limits, indicating compatibility with advanced reasoning training.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Efficiency experiments on A100 GPUs show that NSA\u2019s sparse kernels significantly reduce forward and backward latency versus FlashAttention-2 for long sequences and drastically cut the tokens that must be read from KV cache per decoding step, leading to expected near-linear decoding speedups with sequence length.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "The authors position NSA relative to prior work, arguing that its native, hierarchical, and hardware-aware design overcomes the main issues of earlier sparse attention schemes: phase-restricted sparsity, incompatibility with GQA/MQA, non-differentiable selection, and inefficient training.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "They acknowledge limitations such as focusing only on intra-layer sparsity and using Triton (not fully hand-optimized CUDA), and suggest cross-layer sparsity and further low-level optimization as future work.",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                }
              ],
              "summary": null
            },
            {
              "kind": "explanation.v1",
              "version": "1.0.0",
              "confidence": 0.5,
              "provenance": [],
              "claim": "The paper proposes NSA, a natively trainable, hierarchical sparse attention architecture whose algorithm and kernels are co-designed with modern GPU hardware, enabling substantial end-to-end speedups for long-context LLMs while preserving or improving accuracy relative to full attention and prior sparse approaches.",
              "rationale": "The deep dive traces the argument: the authors analyze why naive or inference-only sparsity underperforms, define NSA\u2019s three-branch attention and remapping mechanisms, describe the Triton kernel tailored to GQA and blockwise access, and present empirical results on pretraining, long-context tasks, reasoning, and efficiency, along with comparisons to related work and design alternatives.",
              "evidence": [
                {
                  "text": "Problem motivation and limitations of existing sparse attention",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "NSA\u2019s overall framework and three-branch sparse attention design",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Detailed algorithmic components: compression, selection, sliding window, and gating",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Kernel design and hardware alignment (arithmetic intensity, GQA, Triton implementation)",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Experimental setup and backbone architecture",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Results on general benchmarks",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Results on long-context benchmarks and retrieval",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Chain-of-thought reasoning results",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Efficiency measurements: training, prefill, and decoding",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Relation to prior sparse attention methods and design choices",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Training behavior and explored but discarded selection strategies",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                },
                {
                  "text": "Limitations and future directions",
                  "source": "paragraphs: b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b13, b14"
                }
              ],
              "summary": null
            }
          ],
          "timeline": [],
          "evolution_narrative": null,
          "glossary": [],
          "relevance_notes": [],
          "review_report": null
        }
      },
      "timestamp": "2025-12-10T04:54:32.147235Z",
      "revision": 13
    }
  ],
  "planner_report": {
    "strategy": "llm_planner.v1",
    "enable_history": false,
    "initial_steps": [
      "parse",
      "translate",
      "jargon",
      "citizen",
      "review",
      "brief"
    ],
    "replan_steps": null,
    "refine_used": false,
    "replan_reason": null,
    "notes": "No preview available; PDF with public_translation task indicates technical content likely needing explanation (translate mandatory), jargon extraction, and citizen relevance. Standard flow without timeline as history not requested."
  }
}